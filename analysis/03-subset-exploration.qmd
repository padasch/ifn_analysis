---
title: "Subset Exploration"
editor_options: 
  chunk_output_type: console
---

```{r message=FALSE, warning=FALSE}
# Source files and packages
source(here::here("R/_setup.R"))

# Create today's figure directory
dir_tmp <- here("figures", format(Sys.time(), format = "%Y-%d-%m"))
if (!dir.exists(dir_tmp)) dir.create(dir_tmp, recursive = T, showWarnings = F)
```

## Summary

-   For this subset analysis, we will focus only on sites that were revisited and that have explicit information on the change in circumference (re-measured circumference or indicator why circumference was not re-measured).

## Create Clean Dataset

### Expand data to one tree per row
```{r}
# Get raw data
data <- suppressWarnings(f_get_raw_data_list())
l_raw_data <- data[[1]]
l_metadata <- data[[2]]

# Wrangle coordinates 
df_loc <- f_attach_wgs_coords_to_raw_placette(l_raw_data$placette)
```

```{r}
# Get tree sampling index
idx_tree  <- f_get_tree_index(l_raw_data)

# Get tree dataframe and attach indeces
df_tree <- 
  l_raw_data$arbre |>
  left_join(idx_tree, by = join_by(idp, a)) |> 
  mutate(
    tree_id  = paste0(idp, "_", a),
    visite = NA,
    visite = ifelse(revisit_state == "revisited" & campagne == visit_1, 1, visite),
    visite = ifelse(revisit_state == "revisited" & campagne == visit_2, 2, visite),
    visite = ifelse(revisit_state == "not_revisited", 1, visite),
    visite = ifelse(revisit_state == "newly_sampled", 2, visite)
    ) |>
  relocate(idp, a, tree_id, campagne, visite,
           visit_1, visit_2, revisit_state, veget, 
           veget5, where(is.numeric))
```

```{r, eval=FALSE}
# Make wide location dataset
# Get dataframes stating which variable is sampled how often and when
loc_vars  <- get_measurement_frequency_of_vars(df_loc,  "location")
tree_vars <- get_measurement_frequency_of_vars(df_tree, "tree")

# Widen dataframes
df_loc_wide  <- widen_dataframe(df_loc,  loc_vars,  "location")
df_tree_wide <- widen_dataframe(df_tree, tree_vars, "tree")

# Quality check for duplicates:
if (length(unique(df_loc_wide$idp)) != nrow(df_loc_wide)) {stop("QC FAILED!")}
if (length(unique(df_tree_wide$tree_id)) != nrow(df_tree_wide)) {stop("QC FAILED!")}
```

### Recruitment - Merge Data Structures
```{r, eval=FALSE}
# Correction for newly grown trees. Below, their state is assigned to their first
# measurement but that occurred during the second visit. To keep data consistent,
# we have to add the information on newly_sampled trees to the second visit

# Get index for newly sampled trees
idx <- which(df_tree_wide$revisit_state == "newly_sampled")

# Split datasets
df_split_new  <- df_tree_wide[idx, ]
df_split_keep <- df_tree_wide[-idx, ]

# Overwrite data so that the variables have the same meaning like revisited trees

# - Overwrite veget with veget5 (veget is all "alive" for newly sampled trees),
# because otherwise they would not have grown to be included in the census
df_split_new$veget5 <- df_split_new$veget
df_split_new$veget  <- "0"

# - Overwrite circumference growth
df_split_new$c13_1 <- df_split_new$c13_2
df_split_new$c13_1 <- 0

# - Overwrite sampling campagnes and visits
df_split_new$campagne_1 <- df_split_new$campagne_2 - 5

df_split_new$visit_1 <- df_split_new$campagne_1
df_split_new$visit_2 <- df_split_new$campagne_2

df_split_new$visite_1 <- 1
df_split_new$visite_2 <- 2

# - No need to overwrite additional variables because they are all NA anyways
# df_split_new |> select(where(~!all(is.na(.)))) |> names()

# Update big dataframe
df_tree_wide_cleaned <- bind_rows(df_split_new, df_split_keep)
```

```{r, eval=FALSE}
# Combine and clean dataframes to one large wide one with one tree per row
vars_clean   <- c("idp", "visite_1", "visite_2", "campagne_1", "campagne_2")
df_loc_wide_fin  <- df_loc_wide  |> mutate(across(all_of(vars_clean), factor))
df_tree_wide_fin <- df_tree_wide_cleaned |> mutate(across(all_of(vars_clean), factor))

df_comb <- 
  left_join(
    df_tree_wide_fin, 
    df_loc_wide_fin, 
    by = c(vars_clean))

# Fixing some factorial variables
df_comb$campagne_1 <- as.double(as.character(df_comb$campagne_1))
df_comb$campagne_2 <- as.double(as.character(df_comb$campagne_2))

# Save data
load_or_save_latest_file("df_comb", "save")
```

```{r}
# Load data
load_or_save_latest_file("df_comb", "load")
head(df_comb)
```

### Attach tree information
#### Attach tree state information
```{r}
# Get key-value dictionary
tree_state_dict <- get_tree_state_dictionary()

# Attach dictionary
df_comb_tree_state <- 
  
  # Take updated dataframe
  df_comb |> 
  
  # Attach information on state of tree at first visit
  left_join(
    tree_state_dict |> 
      rename(veget = ign_code,
             tree_state_1 = tree_state,
             alive_but_injured_1 = alive_but_injured,
             mode_of_death_1 = mode_of_death),
    by = join_by(veget)) |> 
  
  # Attach information on state of tree at second visit
  left_join(
    tree_state_dict |> 
      rename(veget5 = ign_code,
             tree_state_2 = tree_state,
             alive_but_injured_2 = alive_but_injured,
             mode_of_death_2 = mode_of_death),
    by = join_by(veget5)) 
```

#### Attach tree class information
```{r}
# Get key-class dictionary
tree_class_dict <- get_tree_class_dictionary(l_raw_data, l_metadata)

# Attach dictionary
df_comb_tree_class <- 
  df_comb_tree_state |> 
  left_join(tree_class_dict |> 
              rename(espar = lvl_french) |> 
              mutate(tree_class = as.factor(tree_class)),
            by = join_by(espar))
```

### Filter for relevant trees
```{r}
df_comb_filtered <-
  df_comb_tree_class |> 
  filter(
    
    # Data for first visits before 2009 is not cleaned yet, so removing it for now
    campagne_1 > 2009,
    
    # Keep revisited trees with a first c13 measurement
    (revisit_state == "revisited" & !is.na(c13_1)) | 
      
    # Keep trees that have been newly sampled and measured
    (revisit_state == "newly_sampled" & !is.na(c13_2)),
    
    # Remove trees that have been dead or cut at first visit already
    !(revisit_state == "revisited" & (tree_state_1 %in% c("dead", "cut"))),
    
    # Remove trees that have not been found again for some reason
    !(revisit_state == "revisited" & veget5 == "N"),
    
    # Keep only trees that belong to the target stand
    !(identical(cible, NA) | identical(cible, 0)),
    
    # Remove trees that are missing xy-coordinates
    !is.na(lat),
    !is.na(lon)
    
    # Trees that were fully measured and not simplified
    # simplif == 0
    ) 
```

### Attach GEO information
```{r}
# Get temporary dataframe
df_tmp <- df_comb_filtered

# Get information on region, based on coordinates
df_geo <- match_coordinates_to_french_region(df_tmp)

# Match region code number with region name
df_comb_geo <- 
  left_join(
    df_tmp,
    df_geo |> 
      rename(dep = CC_2,
             dep_name = NAME_2,
             region_code = GID_1,
             region_name = NAME_1) |> 
      select(dep, dep_name, region_code, region_name) |> 
      distinct(),
    by = join_by(dep)
  )
```

### Calculate Shadow Growth of Circumference

TODO: THIS SHOULD BE DONE EARLIER, ACTUALLY BEFORE THE ASSESSMENT OF REVISITED OR NOT REVISITED! THEN, WE CAN USE THE SHADOW GROWTH FOR TREES THAT HAVE BEEN RE-ASSESSED AS HAVING SURVIVED/DIED/CUT. ELSE, THEY SHOULD BE DROPPED.

#### Cut Trees
```{r}
# Create temporary df for this section
df_tmp <- df_comb_geo

# Split datasets into cut and not cut
idx <- which(df_tmp$tree_state_2 == "cut")

# Do splitting only if index is not empty
if (length(idx) != 0) {
  
  df_edit <- df_tmp[idx, ] 
  df_keep <- df_tmp[-idx, ]  
  
  # Calculate the circumference achieved until a tree was cut
  # Calculation based on `ifn_wood_harvest_removal.pdf`
  # Drop data if growth could not be back-calculated
  df_edit <- 
    df_edit |> 
    mutate(c13_2 = c13_1 + ir5 / 2,
           shadow_growth = TRUE) |> 
    drop_na(c13_2)
  
  # Combine datasets again
  df_comb_shadow_cut <- 
    bind_rows(
      df_edit,
      df_keep |> mutate(shadow_growth = FALSE)
      )
} else {
  df_comb_shadow_cut <- df_tmp
}
```

#### Not re-measured dead trees
```{r}
# Create temporary df
df_tmp <- df_comb_shadow_cut

# Split datasets into cut and not cut
idx <- which(df_tmp$tree_state_2 == "dead" & is.na(df_tmp$c13_2))

# Do splitting only if index is not empty
if (length(idx) != 0) {
  
  df_edit <- df_tmp[idx, ] 
  df_keep <- df_tmp[-idx, ]  
  
  # Calculate the circumference achieved until a tree was cut
  # Calculation based on `ifn_wood_harvest_removal.pdf`
  # Drop data if growth could not be back-calculated
  df_edit <- 
    df_edit |> 
    mutate(c13_2 = c13_1 + ir5 / 2,
           shadow_growth = TRUE) |> 
    drop_na(c13_2)
  
  # Combine datasets again
  df_comb_shadow_dead <- 
    bind_rows(
      df_edit,
      df_keep |> mutate(shadow_growth = FALSE))
} else {
  df_comb_shadow_dead <- df_tmp
}
```

#### Not re-measured alive trees
```{r}
# Create temporary df
df_tmp <- df_comb_shadow_dead

# Split datasets into trees that survived
idx <- which(df_tmp$tree_state_2 == "alive" & is.na(df_tmp$c13_2))

# Do splitting only if index is not empty
if (length(idx) != 0) {
  
  df_edit <- df_tmp[idx, ] 
  df_keep <- df_tmp[-idx, ]  
  
  # Calculate the circumference achieved until a tree was cut
  # Calculation based on `ifn_wood_harvest_removal.pdf`
  # Drop data if growth could not be back-calculated
  df_edit <- 
    df_edit |> 
    mutate(c13_2 = c13_1 + ir5,
           shadow_growth = TRUE) |> 
    drop_na(c13_2)
  
  # Combine datasets again
  df_comb_shadow_alive <- 
    bind_rows(
      df_edit,
      df_keep |> mutate(shadow_growth = FALSE))
} else {
  df_comb_shadow_alive <- df_tmp
}
```

```{r}
# QC
df_comb_shadow_alive |> 
  filter(revisit_state == "newly_sampled") |> 
  relocate(c13_1, c13_2, tree_state_1, tree_state_2, revisit_state, tree_id)
```

### Calculate growth, biomass, etc.
```{r}
# Get temporary df
df_tmp <- df_comb_shadow_alive

# Attache size related variables
df_comb_size <- 
  df_tmp |> 
  # Attach size related variables
  mutate(
    # Add plot size variable
    plot_area = 25^2 * pi / 10^5, # (25m)^2 * pi / 10000 [m^2/ha] = [ha]
    
    # Diameter
    dbh_1 = c13_1 / pi,
    dbh_2 = c13_2 / pi,
    
    # Basal Area
    ba_1  = pi * ( dbh_1 / 2 ) ^ 2 / plot_area,
    ba_2  = pi * ( dbh_2 / 2 ) ^ 2 / plot_area
  ) |> 
  select(-plot_area)
```

#### Quality controls
```{r}
# Remove all alive trees that have a change of -0.05/yr, because that is likely
# and measurement error and trees should not shrink substantially.
df_comb_qc <- 
  df_comb_size |> 
  mutate(c13_change = ( dbh_2 - dbh_1 ) / dbh_1 / 5 ,
         c13_change = round(c13_change * 100)
         ) |>
  filter(c13_change > -5)
```

## Define most dominant genus per department
```{r}
df_dominant_species <-  
  df_comb_qc |> 
  group_by(dep) |> 
  nest() |> 
  mutate(top_genus = map_chr(data,
                                  ~pull(.,genus_lat) |> 
                                    table() |> 
                                    sort(decreasing = TRUE) |> 
                                    names() |> 
                                    head(3) |> 
                                    paste0(collapse = ", ")
                                    )) |> 
  mutate(top_species = map_chr(data,
                                  ~pull(.,species_lat) |> 
                                    table() |> 
                                    sort(decreasing = TRUE) |> 
                                    names() |> 
                                    head(5) |> 
                                    paste0(collapse = ", ")
                                    )) |> 
  select(-data)
```

## Create Maps of Change
### - Species Selection
```{r}
df_comb_qc$genus_lat |> table() |> sort(decreasing = T) |> head(10)
df_comb_qc$species_lat |> table() |> sort(decreasing = T) |> head(10)
```

```{r}
# Subset for given species
selected_species <- "Fagus sylvatica"
species_classification_level <- "species_lat"

selected_species <- "Quercus"
species_classification_level <- "genus_lat"

# Could be made more flexible with family and other levels
if (str_detect(species_classification_level, "species")) {
  idx_majority <- 
    df_dominant_species |> 
    filter(str_detect(top_species, selected_species)) |> 
    pull(dep)
  
} else if (str_detect(species_classification_level, "genus")) {
  idx_majority <- 
    df_dominant_species |> 
    filter(str_detect(top_genus, selected_species)) |> 
    pull(dep)
}

# Make subset
df_species <- 
  df_comb_qc |> 
  filter(str_detect(get(species_classification_level), selected_species))

df_species <- 
  df_comb_qc |> 
  filter(order_lat == "Fagales")

# QC
if (nrow(df_species) == 0) stop("Empty df!")
```

#### Mortality
##### Stem-based
```{r}
# Filter criteria:
# - Revisited trees that were alive at first visit and dead at second visit
# - Recruited trees that were dead
# - Remove trees that were cut at second visit
df_dead <- 
  df_species |>
  filter(
    (revisit_state == "revisited" & 
       tree_state_1  == "alive" &
       tree_state_2 != "cut") |
    (revisit_state == "newly_sampled")
    )

# ______________________________________________________________________________
# Aggregate data to department level
# - N_init = All trees that were alive at first cencsus
# - N_surv = All trees that were not dead at second census

df_dep <- 
  df_dead |> 
  group_by(campagne_1, dep) |> 
  nest() |> 
  ungroup() |> 
  mutate(n_init    = map_dbl(data, ~nrow(.)),
         n_surv    = map_dbl(data, ~filter(., tree_state_2 == "alive") |> nrow()),
         change = (1 - (n_surv/n_init) ^ (1/5) ) * 100
         )

df_dep_reduced <-
  df_dep |> 
  mutate(census_interval = paste0(campagne_1, "-", campagne_1 + 5),
         census_interval = as.factor(census_interval)) |> 
  select(-data, -n_init, -n_surv, -campagne_1)

# ______________________________________________________________________________
# Cleaning Steps
# Set mortality to NA if species is not dominant in the given department
df_filtered <- 
  df_dep_reduced |>
  mutate(change = ifelse(dep %in% idx_majority, change, NA))

# TODO BUGFIX: One site has a 20% mortality, which messes with the color-scale.
# I am excluding that data for now.
df_filtered <- df_filtered |> filter(change < 10)

# ______________________________________________________________________________
# Match data to regions
all_deps      <- df_comb_qc |> pull(dep) |> unique()
dummy         <- c(2015:2021)
all_intervals <- paste0(dummy - 5, "-", dummy)

df_dep_grid   <-
  expand.grid(all_deps, all_intervals) |> 
  rename(
  dep = Var1,
  census_interval = Var2
  ) |> 
  as_tibble()

df_dep_grid <- 
  left_join(
    df_dep_grid, 
    df_filtered,
    by = join_by(dep, census_interval))

# Plot mortality
# Get shapefile
sf_france <- get_shapefile_france()

# Attach data
df_plot <- 
  right_join(
    sf_france |> rename(dep = CC_2), 
    df_dep_grid,
    by = "dep")

# ______________________________________________________________________________
# Plot it
p <- 
  ggplot() +
  geom_sf(data = df_plot, 
          aes(fill = change)) +
  facet_wrap(~census_interval) +
  scale_fill_viridis_c(
    option = "A", 
    na.value = "white",
    name = "%-stems lost per year: \n"
    ) +
  labs(title = paste0("Mortality of ", selected_species),
       subtitle = 
         expression(
           paste("Stem-based mortality following Esquivel-Muelbert et al. 2020: ",
           m == (1 - ( N[t1] / N[t0] ) ^ (1/T)) %*% 100)
           ),
       caption = paste0(
         "\n Subset:\n",
         "- Trees that were alive at first visit and were dead at second visit. \n",
         "- Trees that were cut at second visit are not included to show natural mortality."
         ) 
       ) +
  theme_classic() +
  theme(plot.caption = element_text(hjust = 0),
        legend.position = "bottom")

# Save it
ggsave(
  paste0(
    "figures/", format(Sys.time(), format = "%Y-%d-%m"), "/",
    format(Sys.time(), format = "%H%M%S"), "_plot_",
    selected_species, "-mortality-stem_based.pdf"),
  p,
  height = 10,
  width = 10
)
```

##### Basal area-based
```{r}
# Filter criteria:
# - Revisited trees that were alive at first visit and dead at second visit
# - Remove recruited trees, because only BA measurement at second census
# - Remove trees that were cut at second visit

df_dead <- 
  df_species |>
  filter(
    revisit_state == "revisited",
    tree_state_1  == "alive",
    tree_state_2 != "cut"
    )

# ______________________________________________________________________________
# Aggregate data to department level
# - total_b_0 = Total basal area measured at first census
# - total_b_1 = Total basal area measured at second census

# Aggregate data to department level
df_dep <- 
  df_dead |> 
  group_by(campagne_1, dep) |> 
  nest() |> 
  ungroup() |> 
  mutate(
    sample_area_hacres = map_dbl(data,
                                 ~pull(., idp) |> 
                                   unique() |> 
                                   length() * 
                                   25^2*pi/10^5), 
    total_ba_0 = map_dbl(data, 
                         ~pull(., ba_1) |> 
                           sum()),
    total_ba_1 = map_dbl(data, 
                         ~filter(., tree_state_2 != "dead") |> 
                           pull(ba_1) |> 
                           sum()),
    change = log(total_ba_0) - log(total_ba_1) / 5,
    # change = (total_ba_1 - total_ba_0) / total_ba_0 / 5 * 100,
    # change_ha_yr = (total_ba_1 - total_ba_0) / 5 / sample_area_hacres
    )

df_dep_reduced <-
  df_dep |> 
  mutate(census_interval = paste0(campagne_1, "-", campagne_1 + 5),
         census_interval = as.factor(census_interval)) |> 
  select(dep, census_interval, change)

# ______________________________________________________________________________
# Cleaning Steps
# Set mortality to NA if species is not dominant in the given department
df_filtered <- 
  df_dep_reduced |>
  mutate(change = ifelse(dep %in% idx_majority, change, NA))

# Filter out non-valid values
# Remove negative mortality, because it is growth and makes no sense
# df_filtered <- df_dep_reduced |> filter(change > 0)
df_filtered <-
  df_filtered |> 
  mutate(change = ifelse(change > 0, change, NA))

# ______________________________________________________________________________
# Match data to regions
all_deps      <- df_comb_qc |> pull(dep) |> unique()
dummy         <- c(2015:2021)
all_intervals <- paste0(dummy - 5, "-", dummy)

df_dep_grid   <-
  expand.grid(all_deps, all_intervals) |> 
  rename(
  dep = Var1,
  census_interval = Var2
  ) |> 
  as_tibble()

df_dep_grid <- 
  left_join(
    df_dep_grid, 
    df_filtered,
    by = join_by(dep, census_interval))

# Plot mortality
# Get shapefile
sf_france <- get_shapefile_france()

# Attach data
df_plot <- 
  right_join(
    sf_france |> rename(dep = CC_2), 
    df_dep_grid,
    by = "dep")

# ______________________________________________________________________________
# Plot it
p <- 
  ggplot() +
  geom_sf(data = df_plot, aes(fill = change)) +
  facet_wrap(~census_interval) +
  scale_fill_viridis_c(
    direction = -1,
    option = "A", 
    na.value = "white",
    name = "% of basal area lost per year: \n"
    ) +
  labs(title = paste0("Mortality of ", selected_species)
       # ,subtitle = 
       #   expression(
       #     paste("Mortality based on change in total basal area per department: ",
       #     m == (BA[t1] - BA[t1]) / BA[t0] %/% 5 %*% 100)
       #     )
       # ,caption = paste0(
       #   "\n Subset:\n",
       #   "- Trees that were alive at first visit and were dead at second visit. \n",
       #   "- Trees that were cut at second visit are not included to show natural mortality."
       #   ) 
       ) +
  theme_classic() +
  theme(plot.caption = element_text(hjust = 0),
        legend.position = "bottom")

# Save it
ggsave(
  paste0(
    "figures/", format(Sys.time(), format = "%Y-%d-%m"), "/",
    format(Sys.time(), format = "%H%M%S"), "_plot_",
    selected_species, "-mortality-basal_area_based.pdf"),
  p,
  height = 10,
  width = 10
)
```

#### Growth
```{r}
# Filter criteria:
# - Revisited trees that were alive at first visit and second visit
# - New trees that were recruited and alive
df_growth <- 
  df_species |>
  filter(
    (revisit_state == "revisited" &
       tree_state_1  == "alive" & 
       tree_state_2  == "alive") |
    (revisit_state == "newly_sampled" &
     tree_state_1  == "alive")
    )

# ______________________________________________________________________________
# Aggregate data to department level
# - total_b_0 = Total basal area measured at first census
# - total_b_1 = Total basal area measured at second census

# Aggregate data to department level
df_dep <- 
  df_growth |> 
  group_by(campagne_1, dep) |> 
  nest() |> 
  ungroup() |> 
  mutate(total_ba_0 = map_dbl(data, ~pull(., ba_1) |> sum()),
         total_ba_1 = map_dbl(data, ~pull(., ba_2) |> sum()),
         change = (total_ba_1 - total_ba_0) / total_ba_0 / 5 * 100
         )

df_dep_reduced <-
  df_dep |> 
  mutate(census_interval = paste0(campagne_1, "-", campagne_1 + 5),
         census_interval = as.factor(census_interval)) |> 
  select(dep, census_interval, change)

# ______________________________________________________________________________
# Cleaning Steps
# Set mortality to NA if species is not dominant in the given department
df_filtered <- 
  df_dep_reduced |>
  mutate(change = ifelse(dep %in% idx_majority, change, NA))

# Filter out non-valid values
# TODO: There is one region with massive growth, which distorts the coloring
# Removing this for now...
df_dep_reduced |> arrange(desc(change))
df_filtered <- df_filtered |> filter(change < 10)

# ______________________________________________________________________________
# Match data to regions
all_deps      <- df_comb_qc |> pull(dep) |> unique()
dummy         <- c(2015:2021)
all_intervals <- paste0(dummy - 5, "-", dummy)

df_dep_grid   <-
  expand.grid(all_deps, all_intervals) |> 
  rename(
  dep = Var1,
  census_interval = Var2
  ) |> 
  as_tibble()

df_dep_grid <- 
  left_join(
    df_dep_grid, 
    df_filtered,
    by = join_by(dep, census_interval))

# Plot change
# Get shapefile
sf_france <- get_shapefile_france()

# Attach data
df_plot <- 
  right_join(
    sf_france |> rename(dep = CC_2), 
    df_dep_grid,
    by = "dep")

# ______________________________________________________________________________
# Plot it
p <- 
  ggplot() +
  geom_sf(data = df_plot, aes(fill = change)) +
  facet_wrap(~census_interval) +
  scale_fill_viridis_c(
    option = "A", 
    na.value = "white",
    name = "% of basal area gained per year: \n"
    ) +
  labs(title = paste0("Growth of ", selected_species),
       subtitle = 
         expression(
           paste("Growth based on change in total basal area per department: ",
           g == ( BA[t1] - BA[t0]) / BA[t0] %/% 5 %*% 100)
           ),
       caption = paste0(
         "\n Subset:\n",
         "- Fagales trees that were alive at first visit and second visit. \n",
         "- Fagales trees that were recruited at second visit \n"
         ) 
       ) +
  theme_classic() +
  theme(plot.caption = element_text(hjust = 0),
        legend.position = "bottom")

# Save it
ggsave(
  paste0(
    "figures/", format(Sys.time(), format = "%Y-%d-%m"), "/",
    format(Sys.time(), format = "%H%M%S"), "_plot_",
    selected_species, "-growth-basal_area_based.pdf"),
  p,
  height = 10,
  width = 10
)
```

#### Removal
##### Stem-based
```{r}
# Filter criteria:
# - Revisited trees that were alive at first visit and cut at second visit
# - Remove trees that were dead at second visit

df_cut <- 
  df_species |>
  filter(
    (revisit_state == "revisited" & 
       tree_state_1  == "alive" &
       tree_state_2  != "dead") |
    (revisit_state == "newly_sampled")
    )

# ______________________________________________________________________________
# Aggregate data to department level
# - N_init = All trees that were alive at first cencsus
# - N_surv = All trees that were not removed at second census

df_dep <- 
  df_cut |> 
  group_by(campagne_1, dep) |> 
  nest() |> 
  ungroup() |> 
  mutate(n_init    = map_dbl(data, ~nrow(.)),
         n_surv    = map_dbl(data, ~filter(., tree_state_2 == "alive") |> nrow()),
         # change = (1 - (n_surv/n_init) ^ (1/5) ) * 100
         change = (1 - (n_surv/n_init) ^ (1/5) ) * 100
         )

df_dep_reduced <-
  df_dep |> 
  mutate(census_interval = paste0(campagne_1, "-", campagne_1 + 5),
         census_interval = as.factor(census_interval)) |> 
  select(-data, -n_init, -n_surv, -campagne_1)

# ______________________________________________________________________________
# Cleaning Steps
# Set mortality to NA if species is not dominant in the given department
df_filtered <- 
  df_dep_reduced |>
  mutate(change = ifelse(dep %in% idx_majority, change, NA))

# TODO BUGFIX: One site has a 20% mortality, which messes with the color-scale.
# I am excluding that data for now.
df_dep_reduced |> arrange(desc(change))
df_filtered <- df_filtered |> filter(change < 10)

# ______________________________________________________________________________
# Match data to regions
all_deps      <- df_comb_qc |> pull(dep) |> unique()
dummy         <- c(2015:2021)
all_intervals <- paste0(dummy - 5, "-", dummy)


df_dep_grid   <-
  expand.grid(all_deps, all_intervals) |> 
  rename(
  dep = Var1,
  census_interval = Var2
  ) |> 
  as_tibble()

df_dep_grid <- 
  left_join(
    df_dep_grid, 
    df_filtered,
    by = join_by(dep, census_interval))

# Plot mortality
# Get shapefile
sf_france <- get_shapefile_france()

# Attach data
df_plot <- 
  right_join(
    sf_france |> rename(dep = CC_2), 
    df_dep_grid,
    by = "dep")


# ______________________________________________________________________________
# Plot it
p <- 
  ggplot() +
  geom_sf(data = df_plot, 
          aes(fill = change)) +
  facet_wrap(~census_interval) +
  scale_fill_viridis_c(
    option = "A", 
    na.value = "white",
    name = "%-stems removed per year: \n"
    ) +
  labs(title = paste0("Removal of ", selected_species),
       subtitle = 
         expression(
           paste("Stem-based removal following Esquivel-Muelbert et al. 2020: ",
           m == (1 - ( N[t1] / N[t0] ) ^ (1/T)) %*% 100)
           ),
       caption = paste0(
         "\n Subset:\n",
         "- Trees that were alive at first visit and were removed at second visit. \n",
         "- Trees that were dead at second visit are not included to show only removal."
         ) 
       ) +
  theme_classic() +
  theme(plot.caption = element_text(hjust = 0),
        legend.position = "bottom")

# Save it
ggsave(
  paste0(
    "figures/", format(Sys.time(), format = "%Y-%d-%m"), "/",
    format(Sys.time(), format = "%H%M%S"), "_plot_",
    selected_species, "-removal-stem_based.pdf"),
  p,
  height = 10,
  width = 10
)
```

##### Basal area-based

TODO: THIS IS NOT WORKING YET!!!

```{r eval=FALSE}
# Filter criteria:
# - Revisited trees that were alive at first visit and dead at second visit
# - Remove recruited trees, because only BA measurement at second census
# - Remove trees that were cut at second visit

df_cut <- 
  df_species |>
  filter(
    revisit_state == "revisited",
    tree_state_1  == "alive",
    tree_state_2 != "dead"
    )

# ______________________________________________________________________________
# Aggregate data to department level
# - total_b_0 = Total basal area measured at first census
# - total_b_1 = Total basal area measured at second census

# Aggregate data to department level
df_dep <- 
  df_cut |> 
  group_by(campagne_1, dep) |> 
  nest() |> 
  ungroup() |> 
  mutate(total_ba_0 = map_dbl(data, ~pull(., ba_1) |> sum()),
         total_ba_1 = map_dbl(data, ~filter(., tree_state_2 == "alive") |> 
                                pull(ba_2) |> 
                                sum()),
         change = (total_ba_1 - total_ba_0) / total_ba_0 / 5 * 100
         )

df_dep_reduced <-
  df_dep |> 
  mutate(census_interval = paste0(campagne_1, "-", campagne_1 + 5),
         census_interval = as.factor(census_interval)) |> 
  select(dep, census_interval, change)

# ______________________________________________________________________________
# Filter out non-valid values
# Remove negative mortality, because it is growth and makes no sense
df_dep_reduced |> arrange(change)
df_filtered <- df_dep_reduced |> filter(change > 0)

# ______________________________________________________________________________
# Match data to regions
all_deps      <- df_comb_qc |> pull(dep) |> unique()
dummy         <- c(2015:2021)
all_intervals <- paste0(dummy - 5, "-", dummy)

df_dep_grid   <-
  expand.grid(all_deps, all_intervals) |> 
  rename(
  dep = Var1,
  census_interval = Var2
  ) |> 
  as_tibble()

df_dep_grid <- 
  left_join(
    df_dep_grid, 
    df_filtered,
    by = join_by(dep, census_interval))

# Plot mortality
# Get shapefile
sf_france <- get_shapefile_france()

# Attach data
df_plot <- 
  right_join(
    sf_france |> rename(dep = CC_2), 
    df_dep_grid,
    by = "dep")

# ______________________________________________________________________________
# Plot it
p <- 
  ggplot() +
  geom_sf(data = df_plot, aes(fill = change)) +
  facet_wrap(~census_interval) +
  scale_fill_viridis_c(
    option = "A", 
    na.value = "white",
    name = "% of basal area removed per year: \n"
    ) +
  labs(title = paste0("Mortality of ", selected_species),
       subtitle = 
         expression(
           paste("Mortality based on change in total basal area per department: ",
           m == (BA[t1] - BA[t1]) / BA[t0] %/% 5 %*% 100)
           ),
       caption = paste0(
         "\n Subset:\n",
         "- Trees that were alive at first visit and were dead at second visit. \n",
         "- Trees that were cut at second visit are not included to show natural mortality."
         ) 
       ) +
  theme_classic() +
  theme(plot.caption = element_text(hjust = 0),
        legend.position = "bottom")

# Save it
ggsave(
  paste0(
    "figures/", format(Sys.time(), format = "%Y-%d-%m"), "/",
    format(Sys.time(), format = "%H%M%S"), "_plot_",
    selected_species, "-removal-basal_area_based.pdf"),
  p,
  height = 10,
  width = 10
)
```

# --- WIP

## New approach to calcualte growth and mortality

```{r}
hexmap <- 

df_tmp <- df_species

# Filter:
# - Keep only trees that were alive at 1st census and not cut at 2nd census
# - Filter for sites that have at least five trees of the species
df_tmp <- 
  df_tmp |> 
  filter(tree_state_1 == "alive", tree_state_2 != "cut") |> 
  group_by(idp, campagne_1) |> 
  filter(n() > 4) |> 
  nest()
```


```{r}
df_tmp <- df_species

# Filter:
# - Keep only trees that were alive at 1st census and not cut at 2nd census
# - Filter for sites that have at least five trees of the species
df_tmp <- 
  df_tmp |> 
  filter(tree_state_1 == "alive", tree_state_2 != "cut") |> 
  group_by(idp, campagne_1) |> 
  filter(n() > 4) |> 
  nest()

# Calculate site-level changes
tic()
df_tmp <- 
  df_tmp |> 
  mutate(
    
    n_ini = map_dbl(
      data, ~filter(., revisit_state == "revisited") |>
        nrow(),
      .progress = T
      ),
    
    n_fin = map_dbl(
      data, ~filter(., tree_state_2 == "alive") |>
        nrow(),
      .progress = T
      ),
    
    n_sur = map_dbl(
      data, ~filter(., revisit_state == "revisited", tree_state_2  == "alive") |>
        nrow(),
      .progress = T
      ),
    
    ba_ini = map_dbl(
      data, ~filter(., revisit_state == "revisited") |>
        pull(ba_1) |> sum(),
      .progress = T
      ),
    
    ba_fin = map_dbl(
      data, ~filter(., tree_state_2 == "alive") |>
        pull(ba_2) |> sum(),
      .progress = T
      ),
    
    ba_sur_0 = map_dbl(
      data, ~filter(., revisit_state == "revisited", tree_state_2  == "alive") |>
        pull(ba_1) |> sum(),
      .progress = T
      ),
    
    ba_sur_1 = map_dbl(
      data, ~filter(., revisit_state == "revisited", tree_state_2  == "alive") |>
        pull(ba_2) |> sum(),
      .progress = T
      ),
    
    # n_mor_yr    = log(n_ini / n_sur) / 5,
    # n_rec_yr    = log(n_fin / n_sur) / 5,
    # 
    # ba_loss_yr    = log(ba_ini / ba_sur_0) / 5,
    # ba_gain_yr    = log(ba_fin / ba_sur_0) / 5,
    # ba_ingr_yr    = log(ba_sur_1 / ba_sur_0) / 5,
    # 
    # ba_loss_yr    = (ba_ini - ba_sur_0) / 5, 
    # ba_gain_yr    = (ba_fin - ba_sur_0) / 5
    )
toc()
```

# Hexagonal Maps
### Online Tutorials

Link: https://r-graph-gallery.com/329-hexbin-map-for-distribution.html

```{r}
# Libraries
library(tidyverse)
library(viridis)
library(hrbrthemes)
library(mapdata)

## Original Example ------------------------------------------------------------
# Load dataset from github
data <- read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/17_ListGPSCoordinates.csv", sep=",", header=T)

# plot
data %>%
  filter(homecontinent=='Europe') %>%
  ggplot( aes(x=homelon, y=homelat)) + 
    geom_hex(bins=59) +
    ggplot2::annotate("text", x = -27, y = 72, label="Where people tweet about #Surf", colour = "black", size=5, alpha=1, hjust=0) +
    ggplot2::annotate("segment", x = -27, xend = 10, y = 70, yend = 70, colour = "black", size=0.2, alpha=1) +
    theme_void() +
    xlim(-30, 70) +
    ylim(24, 72) +
    scale_fill_viridis(
      option="B",
      trans = "log", 
      breaks = c(1,7,54,403,3000),
      name="Tweet # recorded in 8 months", 
      guide = guide_legend( keyheight = unit(2.5, units = "mm"), keywidth=unit(10, units = "mm"), label.position = "bottom", title.position = 'top', nrow=1) 
    )  +
    ggtitle( "" ) +
    theme(
      legend.position = c(0.8, 0.09),
      legend.title=element_text(color="black", size=8),
      text = element_text(color = "#22211d"),
      plot.background = element_rect(fill = "#f5f5f2", color = NA), 
      panel.background = element_rect(fill = "#f5f5f2", color = NA), 
      legend.background = element_rect(fill = "#f5f5f2", color = NA),
      plot.title = element_text(size= 13, hjust=0.1, color = "#4e4d47", margin = margin(b = -0.1, t = 0.4, l = 2, unit = "cm")),
    )
## -----------------------------------------------------------------------------
```

```{r}
tmp_perc <- function(v1, v2){
  (v2-v1)/v1
}

## My adaptation of the online tutorial
tmp <- data |> mutate(v1 = 1,
                      v2 = 2)

ppp <- 
  tmp %>%
  filter(homecontinent=='Europe') %>%
  ggplot( aes(x=homelon, y=homelat)) + 
  # geom_hex(bins=59) +
  # stat_summary_hex(bins = 59, aes(z = sum(v1, v2))) +
  stat_summary_hex(bins = 59, fun= ~sum(v1,2)) +
  ggplot2::annotate("text", x = -27, y = 72, label="Where people tweet about #Surf", colour = "black", size=5, alpha=1, hjust=0) +
  ggplot2::annotate("segment", x = -27, xend = 10, y = 70, yend = 70, colour = "black", size=0.2, alpha=1) +
  theme_void() +
  xlim(-30, 70) +
  ylim(24, 72) +
  #  scale_fill_viridis(
  #   option="B",
  #   trans = "log",
  #   # breaks = c(1,7,54,403,3000),
  #   name="Tweet # recorded in 8 months", 
  #   guide = guide_legend( keyheight = unit(2.5, units = "mm"), keywidth=unit(10, units = "mm"), label.position = "bottom", title.position = 'top', nrow=1) 
  # )  +
  ggtitle( "" ) +
  theme(
    legend.position = c(0.8, 0.09),
    legend.title=element_text(color="black", size=8),
    text = element_text(color = "#22211d"),
    plot.background = element_rect(fill = "#f5f5f2", color = NA), 
    panel.background = element_rect(fill = "#f5f5f2", color = NA), 
    legend.background = element_rect(fill = "#f5f5f2", color = NA),
    plot.title = element_text(size= 13, hjust=0.1, color = "#4e4d47", margin = margin(b = -0.1, t = 0.4, l = 2, unit = "cm")),
  )

ppp
```

## Stackoverflow
https://stackoverflow.com/questions/76153155/creating-a-hexagonal-map-from-a-shapefile-in-r

```{r}
library("sf")
library("tidyverse")
library("rnaturalearth")
library("rnaturalearthdata")
```

```{r}
# Original
UK <- ne_countries(scale = "large",
                   country = "United Kingdom",
                   returnclass = "sf") |>
  st_geometry() |> ## only geometry needed to clip the hexagonal grid
  st_transform(27700) ## reproject to British National Grid


hexgrid <- st_make_grid(UK,
               cellsize = 2e4, ## unit: metres; change as required
               what = 'polygons',
               square = FALSE ## !
               ) |>
  st_as_sf()

hexgrid_UK <- hexgrid[c(unlist(st_contains(UK, hexgrid)), 
                        unlist(st_overlaps(UK, hexgrid))) ,]

ggplot() +
  geom_sf(data = hexgrid_UK)
```

```{r}
FR <- ne_countries(scale = "large",
                   geounit = "France",
                   returnclass = "sf") |>
  st_geometry() |> ## only geometry needed to clip the hexagonal grid
  st_transform(27700) ## reproject to British National Grid


hexgridFR <- st_make_grid(FR,
               cellsize = 2e4, ## unit: metres; change as required
               what = 'polygons',
               square = FALSE ## !
               ) |>
  st_as_sf()

hexgrid_FR <- hexgrid[c(unlist(st_contains(FR, hexgridFR)), 
                        unlist(st_overlaps(FR, hexgridFR))) ,]

ggplot() +
  geom_sf(data = hexgrid_FR)
```

## Blogpost

Link: https://strimas.com/post/hexagonal-grids/

```{r}
study_area <- getData("GADM", country = "LKA", level = 0, path = tempdir()) %>% 
  disaggregate %>% 
  geometry
study_area <- sapply(study_area@polygons, slot, "area") %>% 
  {which(. == max(.))} %>% 
  study_area[.]
plot(study_area, col = "grey50", bg = "light blue", axes = TRUE, cex = 20)
text(81.5, 9.5, "Study Area:\nSri Lanka")
```


### My adaptation
```{r}
# df_tmp from above WIP section

df_xxx <- 
  df_tmp |> 
  select(-data) |> 
  ungroup() |> 
  left_join(df_comb_qc |> ungroup() |> select(idp, lat, lon) |> distinct())

df_xxx <- 
  df_xxx |> 
  group_by(lat, lon) |> 
  nest() |> 
  ungroup() |> 
  rename(d = data)
  

tmp_fct <- function(d) {
  n_rec_yr = log(sum(d$n_fin) / sum(d$n_sur)) / 5
}

df_xxx |> 
  ggplot() +
  aes(x = lon, y = lat, z = d) +
  stat_summary_hex(bins = 59, fun = ~tmp_fct(.x))
```



```{r}
# your_data <- df_xxx
# Sample data creation
set.seed(42)  # for reproducibility

# Generate random data
lon <- runif(100, min = 110, max = 120)
lat <- runif(100, min = 220, max = 230)
var1 <- rpois(100, lambda = 5)
var2 <- rpois(100, lambda = 3)

your_data <- data.frame(lon, lat, var1, var2)

# Create hexagon binning
hex_bins <- hexbin(x = your_data$lon, y = your_data$lat, xbins = 10)

# Extract hexagon IDs from hexbin object
hex_ids <- cell2xy(hex_bins)

# Add hexagon IDs to your data
your_data$hex_id <- as.vector(apply(hex_ids, 2, function(x) hexbin::cellNumber(hex_bins, x[1], x[2])))

# Summarize data within each hexagon
summary_data <- your_data %>%
  group_by(hex_id) %>%
  summarise(
    sum_var1 = sum(var1),
    sum_var2 = sum(var2)
  )

# Calculate the ratio of sum_var2 to sum_var1
summary_data$var_ratio <- summary_data$sum_var2 / summary_data$sum_var1

# Merge summary data back to hexagon information
hexagon_data <- hex_bins@hexcoords %>%
  data.frame() %>%
  cbind(summary_data)

# Create hexbin plot
ggplot(hexagon_data, aes(x = x, y = y, fill = var_ratio)) +
  geom_hex(stat = "identity", bins = 10) +
  scale_fill_gradient(low = "blue", high = "red", name = "Variable Ratio") +
  labs(title = "Hexbin Plot with Variable Sum Ratio",
       x = "Longitude", y = "Latitude") +
  theme_minimal()
```


```{r}
library(dplyr)
library(ggplot2)

# Define the custom function for stat_summary_hex
custom_fun <- function(z) {
  sum(z$var2) / sum(z$var1)
}

# Create the hexbin plot
your_data %>%
  ggplot(aes(x = lon, y = lat, z = var1)) +
  stat_summary_hex(fun = custom_fun, bins = 30, color = "white", size = 0.5) +
  labs(title = "Hexbin Plot with Custom Summary Function",
       x = "Longitude", y = "Latitude") +
  theme_minimal()

```


```{r}
# Replace 'path/to/your_shapefile.shp' with the actual path to your shapefile
shapefile_path <- here("data/raw/maps/qgis-hexmap_fr_20231005-1350.shp")

# Load the shapefile
your_shapefile <- st_read(shapefile_path)

# Plot the shapefile
ggplot() +
  geom_sf(data = your_shapefile) +
  ggtitle("Your Shapefile")
```



```{r}
df_tmp2 <- 
  df_tmp |> 
  ungroup() |> 
  mutate(
    region = map_chr(data, ~pull(., region_name) |> unique()),
    region = as.factor(region)) |> 
  select(-data) |> 
  group_by(campagne_1, region) |> 
  nest() |> 
  mutate(
    mean_ba_loss_yr = map_dbl(data, ~pull(., ba_loss_yr) |> mean(na.rm = TRUE)),
    mean_ba_gain_yr = map_dbl(data, ~pull(., ba_gain_yr) |> mean(na.rm = TRUE)),
    mean_n_mor_yr = map_dbl(data, ~pull(., n_mor_yr) |> mean(na.rm = TRUE)),
    mean_n_rec_yr = map_dbl(data, ~pull(., n_rec_yr) |> mean(na.rm = TRUE)),
    
    se_ba_loss_yr = map_dbl(data, ~pull(., ba_loss_yr) |> std_error(na.rm = TRUE)),
    se_ba_gain_yr = map_dbl(data, ~pull(., ba_gain_yr) |> std_error(na.rm = TRUE)),
    se_n_mor_yr = map_dbl(data, ~pull(., n_mor_yr) |> std_error(na.rm = TRUE)),
    se_n_rec_yr = map_dbl(data, ~pull(., n_rec_yr) |> std_error(na.rm = TRUE))
  )
```


```{r}
p <- 
  df_tmp2 |> 
  drop_na() |> 
  arrange(region, campagne_1) |> 
  mutate(campagne_2 = campagne_1 + 5) |> 
  select(-data) |> 
  ggplot() + 
  aes(y = mean_ba_gain_yr, 
      x = mean_ba_loss_yr,
      group = 1,
      fill = as.factor(campagne_2)) +
  # Add layout
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  facet_wrap(~region, scales = "fixed") + 
  
  # Add points
  geom_errorbar(
    aes(ymin = mean_ba_gain_yr - se_ba_gain_yr, 
        ymax = mean_ba_gain_yr + se_ba_gain_yr,
        color = as.factor(campagne_2))
    # width = 0.2  # Width of the error bars
  ) +
  geom_errorbarh(
    aes(xmin = mean_ba_loss_yr - se_ba_loss_yr, 
        xmax = mean_ba_loss_yr + se_ba_loss_yr,
        color = as.factor(campagne_2))
    # width = 0.2  # Width of the error bars
  ) +
  geom_point(color = "black", shape = 21, size = 3) + 
  scale_fill_viridis_d() +
  scale_color_viridis_d() +
  
  # Add arrow paths
  geom_path(aes(alpha = campagne_2),
            arrow = arrow(type = "closed", 
                          length = unit(0.08, "inches"))) +
  scale_alpha_continuous(range = c(0.5, 1)) +  # Adjust the range for desired transparency
  
  # Add theme
  labs(
    title = paste0("Basal Area Trajectories of ", selected_species),
    x = "Mean Basal Area Loss between two census [% / m^2 tree / ha land / yr]",
    y = "Mean Basal Area Gain between two census [% / m^2 tree / ha land / yr]",
    fill = "Year of 2nd Census"
  ) +
  # scale_x_continuous(breaks = c(0, 0.5, 1)) +
  # scale_y_continuous(breaks = c(0, 0.5, 1)) +
  # ylim(0, 1) +
  # xlim(0, 0.5) +
  theme_linedraw() +
  guides(alpha = FALSE, color = FALSE) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
  # theme(legend.position = "right") +
  # guides(alpha = FALSE, fill = guide_legend(nrow = 1)) 

ggsave(
  paste0(
    "figures/", format(Sys.time(), format = "%Y-%d-%m"), "/",
    format(Sys.time(), format = "%H%M%S"), "_plot_",
    selected_species, "_gain-versus-loss-region-level_ABSOLUTE-VALUES.pdf"),
  p,
  height = 10,
  width = 13
)
```


```{r}
p <- 
  df_tmp2 |> 
  drop_na() |> 
  arrange(region, campagne_1) |> 
  mutate(campagne_2 = campagne_1 + 5) |> 
  select(-data) |> 
  ggplot() + 
  aes(y = mean_n_rec_yr, 
      x = mean_n_mor_yr,
      group = 1,
      fill = as.factor(campagne_2)) +
  # Add layout
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  facet_wrap(~region) + 
  
  # Add arrow paths
  geom_path(aes(alpha = campagne_2),
            arrow = arrow(type = "closed", 
                          length = unit(0.08, "inches"))) +
  scale_alpha_continuous(range = c(0.5, 1)) +  # Adjust the range for desired transparency
  
  # Add points
  geom_point(color = "black", shape = 21, size = 2) + 
  scale_fill_viridis_d() +
  
  # Add theme
  labs(
    title = paste0("Number of Trees Trajectories of ", selected_species),
    x = "Mortality [% stems / yr]",
    y = "Recruitment [% stems / yr]",
    fill = "Year of 2nd Census"
  ) +
  # scale_x_continuous(breaks = c(0, 0.5, 1)) +
  # scale_y_continuous(breaks = c(0, 0.5, 1)) +
  ylim(0, 0.02) +
  xlim(0, 0.02) +
  theme_linedraw() +
  guides(alpha = FALSE) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
  # theme(legend.position = "right") +
  # guides(alpha = FALSE, fill = guide_legend(nrow = 1)) 

ggsave(
  paste0(
    "figures/", format(Sys.time(), format = "%Y-%d-%m"), "/",
    format(Sys.time(), format = "%H%M%S"), "_plot_",
    selected_species, "_mortality-versus-recruitment-region-level.pdf"),
  p,
  height = 10,
  width = 13
)
```

# WIP

```{r}
xxx <- 
  df_species |> 
  filter(tree_state_1 == "alive", tree_state_2 != "cut") |> 
  group_by(campagne_1, dep) |> 
  nest() |> 
  ungroup() |> 
  mutate(
    sample_area_hacres = 
      map_dbl(
        data,
        ~pull(., idp) |> unique() |> length() * 25^2*pi/10^5
        ),
    
    ba_0 = 
      map_dbl(
        data, 
        ~pull(., ba_1) |> sum()
        ),
    
    ba_1 = 
      map_dbl(
        data, 
        ~pull(., ba_2) |> sum()
        ),
    
    ba_growth = 
      map_dbl(
        data, 
        ~filter(.,, tree_state_2 == "alive") |> pull(ba_2) |> sum()
        ),
    
    ba_dead_1 = 
      map_dbl(
        data, 
        ~filter(., tree_state_2 == "dead") |> pull(ba_2) |> sum()
        ),
    
    change_ba_prc_yr = (ba_1 - ba_0) / ba_0 / 5,
    growth_ba_prc_yr = ba_growth_1   / ba_0 / 5,
    death_ba_prc_yr  = ba_dead_1     / ba_0 / 5,
    
    growth_ba_yr_ha   = ba_growth_1 / sample_area_hacres,
    death_ba_yr_ha    = ba_dead_1 / sample_area_hacres
    )
```

# Trajectories
```{r}
xxx <- 
  xxx |> 
  group_by(dep) |> 
  mutate(
      growth_ba_yr_ha_scaled = growth_ba_yr_ha / max(growth_ba_yr_ha),
      death_ba_yr_ha_scaled  = death_ba_yr_ha / max(death_ba_yr_ha)
  ) |> 
  ungroup()

xxx |> 
  drop_na() |> 
  arrange(dep, campagne_1) |> 
  mutate(campagne_2 = campagne_1 + 5) |> 
  select(-data) |> 
  ggplot() + 
  aes(y = growth_ba_yr_ha_scaled, 
      x = death_ba_yr_ha_scaled,
      group = 1,
      fill = as.factor(campagne_2)) +
  # Add layout
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  facet_wrap(~dep) + 
  
  # Add arrow paths
  geom_path(aes(alpha = campagne_2),
            arrow = arrow(type = "closed", 
                          length = unit(0.08, "inches"))) +
  scale_alpha_continuous(range = c(0.5, 1)) +  # Adjust the range for desired transparency
  
  # Add points
  geom_point(color = "black", shape = 21, size = 2) + 
  scale_fill_viridis_d() +
  
  # Add theme
  labs(
    title = paste0("Growth versus Mortality Trajectories of ", selected_species),
    x = "Scaled Mortality Rate",
    y = "Scaled Growth Rate",
    fill = "Year of 2nd Census"
  ) +
  scale_x_continuous(breaks = c(0, 0.5, 1)) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  theme_linedraw() +
  theme(legend.position = "top") +
  guides(alpha = FALSE, fill = guide_legend(nrow = 1)) 
```



# --- WIP

### Attach influence information
#### Attach nature influence

```{r} 
# Make temporary dataframe
df_tmp <- df_comb

# DEBUG
set.seed(123) 
df_tmp <- df_comb |> slice_sample(n = 200000)

# Get environmental variables
env_vars <- get_vars("nature")

# Run code - takes very long!
tic()

df_comb_nature <- 
  df_tmp |> 
  nest(data = c(tree_id, env_vars)) |> 
  mutate(env_impact = map(data, ~get_nature_impact_information(.),
                          .progress = TRUE)) |> 
  select(env_impact) |> 
  unnest(env_impact) |> 
  right_join(df_tmp |> select(-env_vars), by = join_by(tree_id))
  
toc()
beep()

# Save data
load_or_save_latest_file("df_comb_nature", "save")
```

#### Attach human influence
```{r}

```


### Classifications of Tree Status, Class, Human / Nature Influence

```{r}


# Classification for human influence

# Classification for natural influence

```


```{r}
# Attach information on vegetation state
df_treid <- 
  df_treid |> 
  left_join(code_veget5 |> rename(veget = lvl),
            by = join_by(veget)) |> 
  left_join(code_veget5 |> rename(veget5 = lvl,
                                  tree_status5 = tree_status),
            by = join_by(veget5))
```

### Break location data to tree-level

```{r}
df_loc_wide <- f_widen_location_data(l_raw_data$placette)
```

### Widen data to one tree per row



I wanted to finally create a uniform approach to widening the tree data but I got stuck with not knowing thiwch variables can be extended and which ones not... Really not sure if it important for going forward to differentiate this. Mostly the data is NA anywas and can be ignored? So, I should just continue with my subset analysis and see if everything works as expected there?

When getting back to this. First try to rerun the code and check if it actually works. I think adding the 'espar' variable to the tree-level data breaks things. Maybe that would then actually be 'location' data. But then again, I should just create a list that I know has to stay constant. And for everything less certain, we widen the data and drop the NAs.

```{r}
# Select variables to widen, to drop, to keep
vars_location_tree <- f_get_fixed_vars_per_level(level = "location_tree")
vars_to_keep       <- df_subset |> select( any_of(vars_location_tree)) |>  names()
vars_to_widen      <- df_subset |> select(-any_of(vars_location_tree)) |>  names()

# Widen dataframe
df_tre_wide <- 
  df_treid |> 
  pivot_wider(
    id_cols = vars_to_keep,
    names_from  = visit_nr,
    values_from = vars_to_widen) |> 
  select(-where(~all(is.na(.))))

# Clean up data frame
## For trees that were sampled the first time in the second campaign, there is 
## the variable veget_2 included (state of vegetation at first sight but second
## sampling campaign). This information can be simply merged into veget_1 without
## loosing information. Also overwrite the state variable for second visit to be 
## the same as for the first visit because, there will be no second visit, and it
## is the latest knowledge on the state. The same applies for "tree_status" variable.
newly_sampled_rows <- df_tre_wide$revisit_state == "newly_sampled"

df_tre_wide$veget_1[newly_sampled_rows]        <- df_tre_wide$veget_2[newly_sampled_rows]
df_tre_wide$veget5_2[newly_sampled_rows]       <- df_tre_wide$veget_2[newly_sampled_rows]
df_tre_wide$tree_status_1[newly_sampled_rows]  <- df_tre_wide$tree_status_2[newly_sampled_rows]
df_tre_wide$tree_status5_2[newly_sampled_rows] <- df_tre_wide$tree_status_2[newly_sampled_rows]

## Clean up names
df_tre_wide_clean <- 
  df_tre_wide |> 
  select(-veget_2, -tree_status_2) |> 
  rename_with(~ gsub("_1$", "_v1", .), ends_with("_1")) |>
  rename_with(~ gsub("_2$", "_v2", .), ends_with("_2")) |> 
  rename(veget_v2 = veget5_v2,
         tree_status_v2 = tree_status5_v2)

## Quality check if widening worked: No tree_id should be doubled:
qc <- length(unique(df_tre_wide_clean$tree_id)) == nrow(df_tre_wide_clean)
if (!qc) {stop("Duplicated tree_id in wide dataframe!")}

# Attach location data and keep only relevant variables
irr_vars <- f_get_list_of_irrelevant_vars()
irr_vars <- c(irr_vars, paste0(irr_vars, "_v2"))

df_comb <- 
  left_join(df_tre_wide_clean, 
            df_loc_wide,
            by = join_by(idp)) |> 
  select(-any_of(irr_vars))

# Attach new variables 
df_fin <- 
  df_comb |> 
  mutate(
    
    # TODO ADD VOLUME
    # TODO ADD IR5 (increment grown within past 5 years)
    
    # Diameter
    dbh_v1 = c13_v1 / pi,
    dbh_v2 = c13_v2 / pi,
    
    # Basal Area
    ba_v1  = pi * ( dbh_v1 / 2 ) ^ 2,
    ba_v2  = pi * ( dbh_v2 / 2 ) ^ 2,
    
    # Changes
    d_c13  = c13_v2 - c13_v1,
    d_htot = htot_v2 - htot_v1,
    d_dbh  = dbh_v2 - dbh_v1,
    d_ba   = ba_v2  - ba_v1
    )
```

# --- WIP

### Filter for first visits 2010 or after

```{r}
# Filter for trees that were visited the first time from 2010 onwards
# meaning the second visit was in 2015.
df_subset <- 
  df_treid |> 
  filter(
    visit_2 > 2014,
    (revisit_state == "newly_sampled" |
       revisit_state == "revisited")
    )

# Make df wide
vars_location_tree <- f_get_fixed_vars_per_level(level = "location_tree")
vars_to_keep       <- df_subset |> select( any_of(vars_location_tree)) |>  names()
vars_to_widen      <- df_subset |> select(-any_of(vars_location_tree)) |>  names()

df_wide <- 
  df_subset |> 
  # arrange(tree_id) |> slice(1:1000) |> 
  pivot_wider(
    id_cols = vars_to_keep,
    names_from  = visit_nr,
    values_from = vars_to_widen) |> 
  select(-where(~all(is.na(.))))

# For trees that were sampled the first time in the second campaign, there is 
# the variable veget_2 included (state of vegetation at first sight but second
# sampling campaign). This information can be simply merged into veget_1 without
# loosing information. Also overwrite the state variable for second visit to be 
# the same as for the first visit because, there will be no second visit, and it
# is the latest knowledge on the state. 
# The same applies for "tree_status" variable.
for (i in 1:nrow(df_wide)) {
  if (df_wide$revisit_state[i] == "newly_sampled") {
    df_wide$veget_1[i]        <- df_wide$veget_2[i]
    df_wide$veget5_2[i]       <- df_wide$veget_2[i]
    df_wide$tree_status_1[i]  <- df_wide$tree_status_2[i]
    df_wide$tree_status5_2[i] <- df_wide$tree_status_2[i]
  }
}

df_wide_fin <- 
  df_wide |> 
  select(-veget_2, -tree_status_2) |> 
  rename_with(~ gsub("_1$", "_v1", .), ends_with("_1")) |>
  rename_with(~ gsub("_2$", "_v2", .), ends_with("_2")) |> 
  rename(veget_v2 = veget5_v2,
         tree_status_v2 = tree_status5_v2)

# Attach location data and keep only relevant variables
irr_vars <- f_get_list_of_irrelevant_vars()
irr_vars <- c(irr_vars, paste0(irr_vars, "_v2"))

# Clean up dataframe and calculate change in c13
df_comb <- 
  left_join(df_wide_fin, 
            df_loc_wide,
            by = join_by(idp)) |> 
  select(-any_of(irr_vars)) |> 
  mutate(d_c13 = c13_v2 - c13_v1)

# Split dataset into re-measured and not re-remeasured
df_measured_1  <- df_comb |> filter( is.na(d_c13)) 
df_measured_2  <- df_comb |> filter(!is.na(d_c13)) 
```

### Wrangle data with only one c13 measurement

#### Trees that were re-visited but not re-measured

TODO: What should be done with the not re-measured trees that were alive or dead?

```{r}
# For trees that were cut at revisit
df_tmp <- 
  df_measured_1 |> 
  filter(revisit_state == "revisited") 

# Set change in c13 at second visit to zero and add 
# to dataframe with two c13 measuremnts
df_attach_cutted_trees <- 
  df_tmp |> 
  filter(tree_status_v2 == "cut" |
         tree_status_v2 == "dead") |> 
  mutate(
    c13_v2 = 0,
    d_c13 = c13_v2 - c13_v1
    )

# Quality controls
df_tmp$tree_status_v2 |> table()
df_attach_cutted_trees$tree_status_v2 |> table()
```

#### Trees that were newly recruited
```{r}
# Get dataframe for newly grown trees
df_tmp <- 
  df_measured_1 |> 
  filter(revisit_state == "newly_sampled") 

# Set change in c13 at second visit to zero and add 
# to dataframe with two c13 measuremnts
df_attach_new_trees <- 
  df_tmp |> 
  mutate(
    c13_v1 = 0,
    d_c13 = c13_v2 - c13_v1
    )

# Quality controls
df_tmp$tree_status_v1 |> table()
df_attach_new_trees$tree_status_v1 |> table()
```

### Create final subset df

```{r}
df_measured_2_ext <- 
  rbind(df_measured_2,
        df_attach_cutted_trees,
        df_attach_new_trees)

# Check that no tree is doubled:
qc <- df_measured_2_ext |> pull(tree_id) |> duplicated() |> all()
if (qc) stop("Duplicated trees in the dataset!")
```

## Subset Plots

### Leaflet
```{r}
# Code adapted from: 
# https://stackoverflow.com/questions/35921590/leaflet-on-r-how-to-create-layers-and-colors-for-each-factor-level-in-dataframe

# Define coordinates for subregion
l_subreg <- list(
  min_lon = 5.542038,
  max_lon = 6.917511,
  min_lat = 45.005973,
  max_lat = 45.556079
)

# Get dataframe of interest:
# df_tmp <- 
#   df_measured_2_ext |>
  

df_tmp <- 
  df_measured_2_ext |> 
  dplyr::filter(between(lon, l_subreg$min_lon, l_subreg$max_lon),
                between(lat, l_subreg$min_lat, l_subreg$max_lat)) |> 
   select(idp, visit_v1, visit_v2, d_c13, tree_status_v2, lat, lon)

df_map <- 
  df_tmp |> 
  nest(data = c(tree_status_v2, d_c13, visit_v1)) |> 
  mutate(
    d_c13   = map_dbl(data, ~mean(.$d_c13)),
    n_trees = map_dbl(data, ~nrow(.)),
    n_dead  = map_dbl(data, ~nrow(filter(., tree_status_v2 == "dead"))),
    n_alive  = map_dbl(data, ~nrow(filter(., tree_status_v2 == "alive"))),
    n_cut  = map_dbl(data, ~nrow(filter(., tree_status_v2 == "cut"))),
    perc_dead  = round(n_dead  / n_trees * 100),
    perc_alive = round(n_alive / n_trees * 100),
    perc_cut   = round(n_cut   / n_trees * 100),
    stand_status = case_when(
                              n_alive > n_cut & n_alive > n_dead ~ "alive",
                              n_cut > n_alive & n_cut > n_dead ~ "cut",
                              n_dead > n_alive & n_dead > n_cut ~ "dead",
                              TRUE ~ "equal"),
         stand_status = as.factor(stand_status)
  )

# Avoid overlaps
df_map$lat <- jitter(df_map$lat, factor = 0.0001)
df_map$lon <- jitter(df_map$lon, factor = 0.0001)

# Define groups for plotting
groups     <-  as.character(unique(df_map$stand_status))
groups_col <-  colorFactor(palette = "viridis", 
                           domain = df_map$stand_status)
map <- 
  leaflet(df_map) |> 
  addProviderTiles(
    providers$Esri.WorldImagery,
    group = "World Imagery") |>
  addProviderTiles(
    providers$Esri.WorldTopoMap, 
    group = "World Topo")

for (g in groups) {
  df_i <-  df_map |> filter(stand_status == g)
  
  map <- 
    map |> 
    addCircleMarkers(
    # addMarkers(
      data = df_i, 
      lng = ~lon, 
      lat = ~lat, 
      color = ~groups_col(stand_status),
      group = g, 
      label = ~paste("Plot ID:", idp,
                     "| 2nd visit: ", visit_v2,
                     "| Stand Status: ", stand_status,
                     "| d c13: ", round(d_c13, 3),
                     "| Trees alive: ", round(n_alive/n_trees * 100), "%",
                     "| Trees dead: ", round(n_cut/n_trees * 100), "%",
                     "| Trees cut: ", round(n_dead/n_trees * 100), "%"),
      opacity = 0.9)
}

map |> 
  addLayersControl(
    overlayGroups = groups,
    baseGroups = c("World Imagery", "World Topo"),
    position = "topleft",
    options = layersControlOptions(collapsed = FALSE)) |> 
  addRectangles(
    lng1 = l_subreg$min_lon, 
    lat1 = l_subreg$min_lat,
    lng2 = l_subreg$max_lon, 
    lat2 = l_subreg$max_lat,
    fillColor = "transparent"
  ) |> 
  addLegend(
    pal = groups_col,
    values = groups,
    title = "Stand Status",
    position = "topleft")
```


### Change in c13
```{r}
#| layout-nrow: 1
data <- data.frame(
  lon = df_measured_2_ext$lon,  # Your longitude values
  lat = df_measured_2_ext$lat,   # Your latitude values
  value = df_measured_2_ext$d_c13       # Your value data
)

n_breaks <- 100

mean_values <- 
  aggregate(
    value ~ cut(lon, breaks = n_breaks) + cut(lat, breaks = n_breaks), 
    data, 
    mean)

colnames(mean_values) <- c("lon", "lat", "value")

ggplot() +
  geom_tile(data = mean_values, aes(x = lon, y = lat, fill = value)) +
  # scale_fill_viridis_c(option = "magma") +
  scico::scale_fill_scico(
    palette = "vikO",
    limits = c(-4, 4)
    ) +
  labs(title = "Change in c13 from 2010-2020",
       caption = "Subset from revisited sites from 2010 onwards. Shown are mean values of binned trees",
       fill  = "Average change in c13 from 2010-2020 [m]: \n") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  theme(legend.position = "bottom",
        plot.title = element_text(hjust = .6),
        plot.subtitle = element_text(hjust = .6),
        plot.caption = element_text(hjust = .6),
        panel.grid = element_blank())

p1 <- 
  ggplot() +
  geom_histogram(data = data, aes(x = value), bins = 250) +
  labs(title = "Counts of delta c13",
       x = "delta c13") +
  geom_vline(xintercept = 0, color = "red") +
  xlim(-3, 2) +
  theme_classic()

p2 <- 
  ggplot() +
  geom_histogram(data = mean_values, aes(x = value), bins = 1000) +
  geom_vline(xintercept = 0, color = "red") +
  labs(subtitle = "Mean values plotted in the map",
       y = NULL,
       x = NULL)  +
  theme_classic()

p1 + inset_element(p2, 0.1, 0.5, 0.5, 0.95)
```

**Locations where trees shrank**

```{r}
#| layout-nrow: 1
#| layout-col: 2

data <- 
  df_measured_2_ext |> 
  select(lon, lat, d_c13) |> 
  rename(value = d_c13) |> 
  filter(value < 0) |> 
  data.frame()

n_breaks <- 100

mean_values <- 
  aggregate(
    value ~ cut(lon, breaks = n_breaks) + cut(lat, breaks = n_breaks), 
    data, 
    mean)

colnames(mean_values) <- c("lon", "lat", "value")

ggplot() +
  geom_tile(data = mean_values, aes(x = lon, y = lat, fill = value)) +
  scale_fill_gradientn(colors = rev(RColorBrewer::brewer.pal(5, "Reds"))) +
  labs(title = "Decrease in c13 from 2010-2020",
       caption = "Subset from revisited sites from 2010 onwards. Shown are mean values of binned trees",
       fill  = "Average change in c13 from 2010-2020 [m]: \n") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  theme(legend.position = "bottom",
        plot.title = element_text(hjust = .6),
        plot.subtitle = element_text(hjust = .6),
        panel.grid = element_blank())
```

```{r}
#| layout-nrow: 1
#| 
## Locations where trees grew
data <- df_measured_2_ext |> 
  select(lon, lat, d_c13) |> 
  rename(value = d_c13) |> 
  filter(value > 0) |> 
  data.frame()

n_breaks <- 100

mean_values <- 
  aggregate(
    value ~ cut(lon, breaks = n_breaks) + cut(lat, breaks = n_breaks), 
    data, 
    mean)

colnames(mean_values) <- c("lon", "lat", "value")

ggplot() +
  geom_tile(data = mean_values, aes(x = lon, y = lat, fill = value)) +
  scale_fill_gradientn(colors = RColorBrewer::brewer.pal(5, "Blues")) +
  labs(title = "Increase in c13 from 2010-2020 (growth and new trees)",
       caption = "Subset from revisited sites from 2010 onwards. Shown are mean values of binned trees",
       fill  = "Average change in c13 from 2010-2020 [m]: \n") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  theme(legend.position = "bottom",
        plot.title = element_text(hjust = .6),
        plot.subtitle = element_text(hjust = .6),
        panel.grid = element_blank())


## Locations where NEW trees grew
data <- 
  df_measured_2_ext |> 
  filter(revisit_state == "newly_sampled") |> 
  select(lon, lat, d_c13) |> 
  rename(value = d_c13) |> 
  filter(value > 0) |> 
  data.frame()

n_breaks <- 100

mean_values <- 
  aggregate(
    value ~ cut(lon, breaks = n_breaks) + cut(lat, breaks = n_breaks), 
    data, 
    mean)

colnames(mean_values) <- c("lon", "lat", "value")

ggplot() +
  geom_tile(data = mean_values, aes(x = lon, y = lat, fill = value)) +
  scale_fill_gradientn(colors = RColorBrewer::brewer.pal(5, "Blues")) +
  labs(title   = "Increase in c13 from 2010-2020 (new trees only)",
       caption = "Subset from revisited sites from 2010 onwards. Shown are mean values of binned trees",
       fill    = "Average change in c13 from 2010-2020 [m]: \n") +
  theme_void() +
  coord_fixed(ratio = 1.3) +
  theme(legend.position = "bottom",
        plot.title = element_text(hjust = .6),
        plot.subtitle = element_text(hjust = .6),
        panel.grid = element_blank())
```


```{r}
knitr::knit_exit()
```

# Subset Analysis

# NO RENDER BELOW ---
