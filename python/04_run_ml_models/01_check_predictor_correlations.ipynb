{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Predictivness of Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Imports\n",
    "\n",
    "# Magic\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualisation\n",
    "from ydata_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# My functions\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from run_mp import *\n",
    "from utilities import *\n",
    "from random_forest_utils import *\n",
    "\n",
    "# Other\n",
    "from os import error\n",
    "import datetime\n",
    "from io import StringIO\n",
    "import re\n",
    "import warnings\n",
    "import chime\n",
    "\n",
    "from pyprojroot import here\n",
    "\n",
    "chime.theme(\"mario\")\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "from sklearn.inspection import PartialDependenceDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_of_interest = \"mort_nat_vol_prc_yr\"  # ! Set Target Variable\n",
    "species_subset = \"all\"  # ! Define species (lowercase)\n",
    "height_class_subset = \"all\"  # ! Define height subset (lowercase)\n",
    "dataset_to_compare_to = \"edo\"  # ! Define predictor database (see below for options)\n",
    "share_of_0s_allowed = 0.01  # ! Set allowed number of 0s in target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor datasets available:\n",
      " 1. apt\n",
      " 2. edo\n",
      " 3. forest_structure-idp\n",
      " 4. dem_derivatives\n",
      " 5. forest_health\n",
      " 6. soil\n",
      " 7. safran\n",
      " 8. growth_mortality\n"
     ]
    }
   ],
   "source": [
    "# Show all available datasets\n",
    "dir_predictors = here(\"data/final/predictor_datasets\")\n",
    "predictor_datasets = [f for f in os.listdir(dir_predictors) if not f.startswith(\".\")]\n",
    "predictor_datasets = [re.sub(\".feather\", \"\", f) for f in predictor_datasets]\n",
    "predictor_datasets = predictor_datasets + [\"growth_mortality\"]\n",
    "print(\"Predictor datasets available:\")\n",
    "for i, dataset in enumerate(predictor_datasets):\n",
    "    print(f\" {i+1}. {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Number of files in directory: 73 | ['species_fraxinus-height_0-10.feather', 'species_acer-height_0-10.feather', 'species_quercus-height_all.feather', 'species_picea-height_10-15.feather', 'species_carpinus-height_10-15.feather', 'species_fagus-height_20-25.feather', 'species_quercus-height_10-15.feather', 'species_abies-height_0-10.feather', 'species_populus-height_20-25.feather', 'species_carpinus-height_0-10.feather']\n"
     ]
    }
   ],
   "source": [
    "# Get files for growth and mortality\n",
    "file_dir = here(\"data/tmp/nfi/growth_and_mortality_data/idp\").as_posix() + \"/\"\n",
    "\n",
    "# List top 10 files\n",
    "file_list = os.listdir(file_dir)\n",
    "print(f\" - Number of files in directory: {len(file_list)} | {file_list[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predictor dataset 'edo': (40022, 317)\n"
     ]
    }
   ],
   "source": [
    "# Load requested predictor dataset\n",
    "if dataset_to_compare_to == \"growth_mortality\":\n",
    "    print(\n",
    "        \"Not loading additional predictor dataset because comparison to growth_mortality data itself.\"\n",
    "    )\n",
    "else:\n",
    "    file_predictors = here(\n",
    "        f\"data/final/predictor_datasets/{dataset_to_compare_to}.feather\"\n",
    "    )\n",
    "    if not os.path.exists(file_predictors):\n",
    "        raise ValueError(f\"Predictor dataset {dataset_to_compare_to} does not exist.\")\n",
    "    else:\n",
    "        df_pr_org = pd.read_feather(file_predictors)\n",
    "        print(\n",
    "            f\"Shape of predictor dataset '{dataset_to_compare_to}': {df_pr_org.shape}\"\n",
    "        )\n",
    "        # display(df_pr_org.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growth & Mortality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✅ Requested file `species_all-height_all.feather` found in directory.\n",
      "     - Shape of data: (40231, 48)\n",
      "     - Variables in data: ['idp', 'n_plots', 'n_a1', 'n_a2', 'n_aa', 'n_ad', 'n_ac', 'n_na', 'ba_ax_v1', 'ba_ax_v2', 'ba_aa_v1', 'ba_aa_v2', 'ba_ad_v1', 'ba_ac_v1', 'ba_na_v2', 'vol_ax_v1', 'vol_aa_v1', 'vol_ad_v1', 'vol_ac_v1', 'mort_tot_stems_prc_yr_esq', 'mort_nat_stems_prc_yr_esq', 'mort_cut_stems_prc_yr_esq', 'mort_tot_stems_prc_yr', 'mort_nat_stems_prc_yr', 'mort_cut_stems_prc_yr', 'mort_tot_ba_yr', 'mort_tot_ba_prc_yr', 'mort_nat_ba_yr', 'mort_nat_ba_prc_yr', 'mort_cut_ba_yr', 'mort_cut_ba_prc_yr', 'mort_tot_vol_yr', 'mort_tot_vol_prc_yr', 'mort_nat_vol_yr', 'mort_nat_vol_prc_yr', 'mort_cut_vol_yr', 'mort_cut_vol_prc_yr', 'grwt_stems_prc_yr', 'grwt_tot_ba_yr', 'grwt_tot_ba_prc_yr', 'grwt_tot_ba_prc_yr_hos', 'grwt_sur_ba_yr', 'grwt_sur_ba_prc_yr', 'grwt_sur_ba_prc_yr_hos', 'grwt_rec_ba_yr', 'grwt_rec_ba_prc_yr', 'change_tot_ba_yr', 'change_tot_ba_prc_yr']\n"
     ]
    }
   ],
   "source": [
    "# Get requested growth and mortality subset\n",
    "file_name = f\"species_{species_subset}-height_{height_class_subset}.feather\"\n",
    "if file_name in file_list:\n",
    "    print(f\" ✅ Requested file `{file_name}` found in directory.\")\n",
    "    df_gm_org = pd.read_feather(file_dir + file_name)\n",
    "    print(f\"     - Shape of data: {df_gm_org.shape}\")\n",
    "    print(f\"     - Variables in data: {df_gm_org.columns.tolist()}\")\n",
    "    # display(df.head())\n",
    "else:\n",
    "    raise ValueError(f\" ❌ Requested file `{file_name}` not found in directory.\")\n",
    "\n",
    "if dataset_to_compare_to == \"growth_mortality\":\n",
    "    df_pr_org = df_gm_org.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove share of 0s in target variable\n",
    "def remove_na_and_reduce_zero_share(df, target_column, max_zero_share, verbose=False):\n",
    "    # Make a copy of the original dataframe\n",
    "    df_org = df.copy()\n",
    "\n",
    "    # Remove NA values\n",
    "    df_nona = df.dropna(subset=[target_column])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Removing NA values from target variable '{target_column}'\")\n",
    "        print(\n",
    "            f\" - Shape of data before removing NA values:\\t {df_org.shape} \\t | % of NAs in target:\\t {(df_org[target_column] == np.nan).mean():.2%}\"\n",
    "        )\n",
    "        print(\n",
    "            f\" - Shape of data after removing NA values:\\t {df_nona.shape} \\t | % of NAs in target:\\t {(df_nona[target_column] == np.nan).mean():.2%}\"\n",
    "        )\n",
    "        print(f\" - Number of NA values removed:\\t\\t\\t {len(df_org) - len(df_nona)}\")\n",
    "\n",
    "    # Calculate the current share of 0 values in the target column\n",
    "    df = df_nona.copy()\n",
    "    zero_share = (df[target_column] == 0).mean()\n",
    "\n",
    "    # Check if the current share exceeds the maximum allowed share\n",
    "    while zero_share > max_zero_share * 1.25:\n",
    "        # Calculate the number of 0 values to remove\n",
    "        num_zeros_to_remove = int((zero_share - max_zero_share) * len(df))\n",
    "\n",
    "        # Get the indices of the 0 values\n",
    "        zero_indices = df[df[target_column] == 0].index\n",
    "\n",
    "        # Randomly select indices to remove\n",
    "        indices_to_remove = np.random.choice(\n",
    "            zero_indices, size=num_zeros_to_remove, replace=False\n",
    "        )\n",
    "\n",
    "        # Remove the selected indices from the dataframe\n",
    "        df = df.drop(indices_to_remove)\n",
    "\n",
    "        # Recalculate the share of 0 values in the target column\n",
    "        zero_share = (df[target_column] == 0).mean()\n",
    "\n",
    "    # Verbose\n",
    "    if verbose:\n",
    "        print(f\"\\nRemoving 0 values from target variable '{target_column}'\")\n",
    "        print(\n",
    "            f\" - Shape of data before removing 0s:\\t {df_nona.shape} \\t | % of 0s in target:\\t {(df_nona[target_column] == 0).mean():.2%}\"\n",
    "        )\n",
    "        print(\n",
    "            f\" - Shape of data after removing 0s:\\t {df.shape} \\t | % of 0s in target:\\t {(df[target_column] == 0).mean():.2%}\"\n",
    "        )\n",
    "        print(f\" - Number of 0s removed:\\t\\t {len(df_nona) - len(df)}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir_base:\t\tcorrelation_exploration/edo/species_all-height_all\n",
      "dir_heatmaps:\t\tcorrelation_exploration/edo/species_all-height_all/heatmaps\n",
      "dir_histogra:\t\tcorrelation_exploration/edo/species_all-height_all/histograms\n",
      "dir_scatters:\t\tcorrelation_exploration/edo/species_all-height_all/mort_nat_vol_prc_yr/scatterplots\n"
     ]
    }
   ],
   "source": [
    "# Set dirs here to avoid changing later\n",
    "dir_base = f\"correlation_exploration/{dataset_to_compare_to}/species_{species_subset}-height_{height_class_subset}\"\n",
    "dir_target = f\"{dir_base}/{target_of_interest}\"\n",
    "dir_heatmaps = f\"{dir_base}/heatmaps\"\n",
    "dir_histogra = f\"{dir_base}/histograms\"\n",
    "dir_scatters = f\"{dir_target}/scatterplots\"\n",
    "dir_rf = f\"{dir_target}/random_forest\"\n",
    "\n",
    "# Make sure all dirs exist\n",
    "os.makedirs(dir_base, exist_ok=True)\n",
    "# os.makedirs(dir_rf, exist_ok=True)  # Created later\n",
    "os.makedirs(dir_heatmaps, exist_ok=True)\n",
    "os.makedirs(dir_histogra, exist_ok=True)\n",
    "os.makedirs(dir_scatters, exist_ok=True)\n",
    "\n",
    "# Print all directories\n",
    "print(f\"dir_base:\\t\\t{dir_base}\")\n",
    "print(f\"dir_heatmaps:\\t\\t{dir_heatmaps}\")\n",
    "print(f\"dir_histogra:\\t\\t{dir_histogra}\")\n",
    "print(f\"dir_scatters:\\t\\t{dir_scatters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore General Correlations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of Predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on figure: 1 | 20\n"
     ]
    }
   ],
   "source": [
    "# Set plot grid size nxn\n",
    "grid_n = 4\n",
    "grid_n_sq = grid_n**2\n",
    "\n",
    "# List of predictors\n",
    "predictors = df_pr_org.columns\n",
    "\n",
    "# Number of figures\n",
    "n_figures = np.ceil(len(predictors) / grid_n_sq)\n",
    "\n",
    "# Ignore divide by zero warning from LOESS\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Create and save figures\n",
    "for i in range(int(n_figures)):\n",
    "    # Verbose\n",
    "    print(f\"Working on figure: {i+1} | {int(n_figures)}\")\n",
    "    # Check if figure already exists\n",
    "    figure_path = os.path.join(dir_histogra, f\"figure_{i+1}.png\")\n",
    "    if os.path.exists(figure_path):\n",
    "        print(f\" ✅ Figure {figure_path} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # ! Create a new figure\n",
    "    fig, axs = plt.subplots(grid_n, grid_n, figsize=(20, 20))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    # Plot each predictor\n",
    "    for j in range(grid_n_sq):\n",
    "        index = i * grid_n_sq + j\n",
    "        if index < len(predictors):\n",
    "            # Get predictor to plot\n",
    "            predictor = predictors[index]\n",
    "            # Verbose\n",
    "            print(f\"    Working on {predictor}:\\t {j+1} | {grid_n_sq}\", end=\"\\t\")\n",
    "\n",
    "            # * Histogram\n",
    "            print(f\"adding histogram...\", end=\"    \")\n",
    "            sns.histplot(\n",
    "                data=df_pr_org,\n",
    "                x=predictor,\n",
    "                ax=axs[j],\n",
    "                color=\"grey\",\n",
    "            )\n",
    "\n",
    "            # * Set the title\n",
    "            axs[j].set_title(f\"Distribution of {predictor}\")\n",
    "\n",
    "        else:\n",
    "            axs[j].axis(\"off\")\n",
    "\n",
    "    # ! Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figure_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Reset warnings\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: grwt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: mort_tot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 18.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: mort_cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 23.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: mort_nat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 16.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get dfs\n",
    "df_pr = df_pr_org.copy()\n",
    "df_gm = df_gm_org.copy()\n",
    "\n",
    "# Check if inputed df is df_gm already, else merge it on idp\n",
    "if dataset_to_compare_to == \"growth_mortality\":\n",
    "    comparing_to_gm = True\n",
    "    df_merged = df_gm\n",
    "    pass\n",
    "else:\n",
    "    comparing_to_gm = False\n",
    "    df_merged = df_gm.merge(df_pr, on=\"idp\", how=\"left\")\n",
    "\n",
    "# Get all meta targets\n",
    "meta_targets = [\"grwt\", \"mort_tot\", \"mort_cut\", \"mort_nat\"]\n",
    "\n",
    "# Loop over all meta targets\n",
    "debug_counter = 0\n",
    "for mt in meta_targets:\n",
    "    print(f\"Working on: {mt}\")\n",
    "    # Get empty correlation df\n",
    "    df_corr = pd.DataFrame()\n",
    "\n",
    "    # Get all sub targets\n",
    "    sub_targets = [c for c in df_merged.columns if c.startswith(mt)]\n",
    "    # Loop over all sub targets\n",
    "    for st in tqdm(sub_targets):\n",
    "        # print(f\"Working on: {st}\")\n",
    "        # Remove NA and 0s from target variable\n",
    "        df_loop = remove_na_and_reduce_zero_share(\n",
    "            df_merged, st, share_of_0s_allowed, verbose=False\n",
    "        )\n",
    "        # Remove other growth_mortality variables if not comparing to it\n",
    "        if not comparing_to_gm:\n",
    "            to_remove = [c for c in df_gm if c != st]\n",
    "            df_loop = df_loop.drop(to_remove, axis=1)\n",
    "        # Calculate correlation between target and all predictors\n",
    "        corr = df_loop.corr()[st].sort_values(ascending=False)\n",
    "        # Save to df with subtarget as column name\n",
    "        df_corr[st] = corr\n",
    "\n",
    "    # ! Create heatmaps\n",
    "    max_rows = 50\n",
    "    num_heatmaps = len(df_corr) // max_rows + 1\n",
    "    last_heatmap_rows = len(df_corr) % max_rows\n",
    "    heatmap_counter = 0\n",
    "    for i in range(num_heatmaps):\n",
    "        heatmap_counter += 1\n",
    "        start_index = i * max_rows\n",
    "        end_index = start_index + max_rows\n",
    "        if i == num_heatmaps - 1 and last_heatmap_rows < max_rows / 2:\n",
    "            figsize = (20, 10)\n",
    "        else:\n",
    "            figsize = (20, 20)\n",
    "        df_corr_subset = df_corr.iloc[start_index:end_index]\n",
    "\n",
    "        # Create heatmap\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        heatmap = ax.pcolor(df_corr_subset, cmap=plt.cm.RdBu, vmin=-1, vmax=1)\n",
    "\n",
    "        # put the major ticks at the middle of each cell\n",
    "        ax.set_xticks(np.arange(df_corr_subset.shape[1]) + 0.5, minor=False)\n",
    "        ax.set_yticks(np.arange(df_corr_subset.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "        # want a more natural, table-like display\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.tick_top()\n",
    "\n",
    "        ax.set_xticklabels(df_corr_subset.columns, minor=False, rotation=45, ha=\"left\")\n",
    "        ax.set_yticklabels(df_corr_subset.index, minor=False)\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(heatmap)\n",
    "\n",
    "        # Add correlation text annotations\n",
    "        for i in range(df_corr_subset.shape[0]):\n",
    "            for j in range(df_corr_subset.shape[1]):\n",
    "                text = ax.text(\n",
    "                    j + 0.5,\n",
    "                    i + 0.5,\n",
    "                    f\"{df_corr_subset.iloc[i, j]:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"black\",\n",
    "                )\n",
    "\n",
    "        # Add title\n",
    "        plt.title(\n",
    "            f\"Correlation between '{dataset_to_compare_to} predictors' and '{mt} targets' ({heatmap_counter}/{num_heatmaps})\\n\",\n",
    "            fontsize=20,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "        # plt.show()\n",
    "        # raise ValueError(\"Stop here\")\n",
    "        plt.savefig(\n",
    "            f\"{dir_heatmaps}/{mt}_{heatmap_counter}.png\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Target Specific Correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing NA values from target variable 'mort_nat_vol_prc_yr'\n",
      " - Shape of data before removing NA values:\t (22900, 48) \t | % of NAs in target:\t 0.00%\n",
      " - Shape of data after removing NA values:\t (22520, 48) \t | % of NAs in target:\t 0.00%\n",
      " - Number of NA values removed:\t\t\t 380\n",
      "\n",
      "Removing 0 values from target variable 'mort_nat_vol_prc_yr'\n",
      " - Shape of data before removing 0s:\t (22520, 48) \t | % of 0s in target:\t 88.13%\n",
      " - Shape of data after removing 0s:\t (2817, 48) \t | % of 0s in target:\t 5.11%\n",
      " - Number of 0s removed:\t\t 19703\n"
     ]
    }
   ],
   "source": [
    "# Get dataframe for predicting on target\n",
    "df_target = pd.merge(df_gm_org[[\"idp\", target_of_interest]], df_pr_org, how=\"left\")\n",
    "\n",
    "# Remove NA and 0s from target variable\n",
    "df_target = remove_na_and_reduce_zero_share(\n",
    "    df_target, target_of_interest, share_of_0s_allowed, verbose=True\n",
    ")\n",
    "\n",
    "# Remove idp index\n",
    "df_target = df_target.drop(\"idp\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✅ File `correlation_exploration/growth_mortality/species_quercus-height_all/mort_nat_vol_prc_yr/density_plot.png` already exists.\n",
      " ❌ File `correlation_exploration/growth_mortality/species_quercus-height_all/mort_nat_vol_prc_yr/histo_plot.png` does not exist. Creating...\n"
     ]
    }
   ],
   "source": [
    "# Create density plot of target variable\n",
    "filename = f\"{dir_target}/density_plot.png\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\" ✅ File `{filename}` already exists.\")\n",
    "else:\n",
    "    print(f\" ❌ File `{filename}` does not exist. Creating...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"Density of {target_of_interest}\")\n",
    "    plt.xlabel(target_of_interest)\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    # Plot the KDE line after adding the text\n",
    "    sns.kdeplot(data=df_target, x=target_of_interest, zorder=1)\n",
    "\n",
    "    # Add ticks along the x-axis to indicate sample count\n",
    "    sample_counts = df_target[target_of_interest].value_counts()\n",
    "    ymax = plt.gca().get_ylim()[1]\n",
    "    for value, _ in sample_counts.items():\n",
    "        plt.text(value, ymax, \"|\", ha=\"center\", va=\"top\", alpha=0.1)\n",
    "\n",
    "    # Save the plot under dir_target\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Create histo plot of target variable\n",
    "filename = f\"{dir_target}/histo_plot.png\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\" ✅ File `{filename}` already exists.\")\n",
    "else:\n",
    "    print(f\" ❌ File `{filename}` does not exist. Creating...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"Density of {target_of_interest}\")\n",
    "    plt.xlabel(target_of_interest)\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    # Plot the KDE line after adding the text\n",
    "    sns.histplot(data=df_target, x=target_of_interest)\n",
    "\n",
    "    # Add ticks along the x-axis to indicate sample count\n",
    "    sample_counts = df_target[target_of_interest].value_counts()\n",
    "    ymax = plt.gca().get_ylim()[1]\n",
    "    for value, _ in sample_counts.items():\n",
    "        plt.text(value, ymax, \"|\", ha=\"center\", va=\"top\", alpha=0.1)\n",
    "\n",
    "    # Save the plot under dir_target\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on figure: 1 | 3\n",
      "    Working on idp:\t 1 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on n_plots:\t 2 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on n_a1:\t 3 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on n_a2:\t 4 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on n_aa:\t 5 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on n_ad:\t 6 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on n_ac:\t 7 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on n_na:\t 8 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on ba_ax_v1:\t 9 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on ba_ax_v2:\t 10 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on ba_aa_v1:\t 11 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on ba_aa_v2:\t 12 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on ba_ad_v1:\t 13 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on ba_ac_v1:\t 14 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on ba_na_v2:\t 15 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on vol_ax_v1:\t 16 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "Working on figure: 2 | 3\n",
      "    Working on vol_aa_v1:\t 1 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on vol_ad_v1:\t 2 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on vol_ac_v1:\t 3 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_tot_stems_prc_yr_esq:\t 4 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_nat_stems_prc_yr_esq:\t 5 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_cut_stems_prc_yr_esq:\t 6 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_tot_stems_prc_yr:\t 7 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_nat_stems_prc_yr:\t 8 | 16\tadding scatterplot...    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pascal/repos/padasch/ifn_analysis/ifna-venv/lib/python3.10/site-packages/matplotlib/axis.py:154: ResourceWarning: unclosed file <_io.BufferedRandom name='correlation_exploration/growth_mortality/species_quercus-height_all/mort_nat_vol_prc_yr/scatterplots/figure_3.png'>\n",
      "  grid_kw = {k[5:]: v for k, v in kwargs.items()}\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding LEOSS...    adding corr...\n",
      "    Working on mort_cut_stems_prc_yr:\t 9 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_tot_ba_yr:\t 10 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_tot_ba_prc_yr:\t 11 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_nat_ba_yr:\t 12 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_nat_ba_prc_yr:\t 13 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_cut_ba_yr:\t 14 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_cut_ba_prc_yr:\t 15 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_tot_vol_yr:\t 16 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "Working on figure: 3 | 3\n",
      "    Working on mort_tot_vol_prc_yr:\t 1 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_nat_vol_yr:\t 2 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_cut_vol_yr:\t 3 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on mort_cut_vol_prc_yr:\t 4 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on grwt_stems_prc_yr:\t 5 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on grwt_tot_ba_yr:\t 6 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on grwt_tot_ba_prc_yr:\t 7 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on grwt_tot_ba_prc_yr_hos:\t 8 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on grwt_sur_ba_yr:\t 9 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on grwt_sur_ba_prc_yr:\t 10 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on grwt_sur_ba_prc_yr_hos:\t 11 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on grwt_rec_ba_yr:\t 12 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on grwt_rec_ba_prc_yr:\t 13 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on change_tot_ba_yr:\t 14 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n",
      "    Working on change_tot_ba_prc_yr:\t 15 | 16\tadding scatterplot...    adding LEOSS...    adding corr...\n"
     ]
    }
   ],
   "source": [
    "# Set plot grid size nxn\n",
    "grid_n = 4\n",
    "grid_n_sq = grid_n**2\n",
    "\n",
    "# List of predictors\n",
    "predictors = [col for col in df_merged.columns if col != target_of_interest]\n",
    "\n",
    "# Number of figures\n",
    "n_figures = np.ceil(len(predictors) / grid_n_sq)\n",
    "\n",
    "# y-axis limits\n",
    "y_max = np.percentile(df_merged[target_of_interest].dropna(), 90)\n",
    "y_min = min(0, np.percentile(df_merged[target_of_interest].dropna(), 10))\n",
    "\n",
    "# Ignore divide by zero warning from LOESS\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Create and save figures\n",
    "for i in range(int(n_figures)):\n",
    "    # Verbose\n",
    "    print(f\"Working on figure: {i+1} | {int(n_figures)}\")\n",
    "    # Check if figure already exists\n",
    "    figure_path = os.path.join(dir_scatters, f\"figure_{i+1}.png\")\n",
    "    if os.path.exists(figure_path):\n",
    "        print(f\" ✅ Figure {figure_path} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # ! Create a new figure\n",
    "    fig, axs = plt.subplots(grid_n, grid_n, figsize=(20, 20))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    # Plot each predictor\n",
    "    for j in range(grid_n_sq):\n",
    "        index = i * grid_n_sq + j\n",
    "        if index < len(predictors):\n",
    "            # Get predictor to plot\n",
    "            predictor = predictors[index]\n",
    "            # Verbose\n",
    "            print(f\"    Working on {predictor}:\\t {j+1} | {grid_n_sq}\", end=\"\\t\")\n",
    "\n",
    "            # Check for NaN values in predictor and target variables\n",
    "            df_loop = df_merged.dropna(subset=[target_of_interest, predictor])[\n",
    "                [target_of_interest, predictor]\n",
    "            ]\n",
    "\n",
    "            # * KDE plot\n",
    "            # print(f\"adding KDE...\", end=\"    \")\n",
    "            # sns.kdeplot(\n",
    "            #     data=df_loop,\n",
    "            #     x=predictor,\n",
    "            #     y=target_of_interest,\n",
    "            #     ax=axs[j],\n",
    "            #     fill=True,\n",
    "            #     warn_singular=False,\n",
    "            #     # levels=100,\n",
    "            #     # thresh=0,\n",
    "            # )\n",
    "\n",
    "            # * Scatterplot (much faster)\n",
    "            print(f\"adding scatterplot...\", end=\"    \")\n",
    "            sns.scatterplot(\n",
    "                data=df_loop,\n",
    "                x=predictor,\n",
    "                y=target_of_interest,\n",
    "                ax=axs[j],\n",
    "                alpha=0.1,\n",
    "                # size=1,\n",
    "                linewidth=0,\n",
    "                color=\"black\",\n",
    "            )\n",
    "\n",
    "            # * LOESS smoother\n",
    "            print(f\"adding LEOSS...\", end=\"    \")\n",
    "            lowess = sm.nonparametric.lowess(\n",
    "                df_loop[target_of_interest], df_loop[predictor], frac=0.30\n",
    "            )\n",
    "            axs[j].plot(lowess[:, 0], lowess[:, 1], color=\"red\")\n",
    "\n",
    "            # * Pearson correlation\n",
    "            print(f\"adding corr...\")\n",
    "            r, _ = pearsonr(df_loop[predictor], df_loop[target_of_interest])\n",
    "            axs[j].set_title(f\"{target_of_interest} ~ {predictor}\\nPearson r: {r:.2f}\")\n",
    "\n",
    "            # * Set the axis limits\n",
    "            x_max = np.percentile(df_loop[predictor], 99)\n",
    "            x_min = min(0, np.percentile(df_loop[predictor], 1))\n",
    "            axs[j].set_xlim(xmin=x_min, xmax=x_max)\n",
    "            axs[j].set_ylim(ymin=y_min, ymax=y_max)\n",
    "        else:\n",
    "            axs[j].axis(\"off\")\n",
    "\n",
    "    # ! Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figure_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Reset warnings\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a small random forest to get feature importances for all variables\n",
    "def run_rf(df_in, target_of_interest, dir_rf):\n",
    "    os.makedirs(dir_rf, exist_ok=True)\n",
    "    # ! Data Preparation ------------------------------------------------------\n",
    "    print(f\" - Prepare data...\")\n",
    "    # Remove NAs\n",
    "    df_in = df_in.dropna(subset=target_of_interest)\n",
    "\n",
    "    # Get X and y\n",
    "    X = df_in.drop(target_of_interest, axis=1)\n",
    "    y = df_in[target_of_interest]\n",
    "\n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=True, random_state=42\n",
    "    )\n",
    "\n",
    "    # Replace NA values in train data with mean\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_train.mean())\n",
    "\n",
    "    # ! Fit ------------------------------------------------------\n",
    "    print(f\" - Fit random forest...\")\n",
    "    # Fit a random forest\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=5, n_jobs=9)\n",
    "    # Fit the model\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # ! Evaluate ------------------------------------------------------\n",
    "    print(f\" - Feature Importance...\")\n",
    "    # Get feature importances\n",
    "    show_top_predictors(X_train, rf_model=rf, current_dir=dir_rf)\n",
    "\n",
    "    print(f\" - Partial Dependence...\")\n",
    "    # Get the top 10 predictors\n",
    "    feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "    top_predictors = feature_importances[:10].index.tolist()\n",
    "    # Create a 2x5 grid for subplots\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "    # Iterate over the top predictors and plot the PDP for each one\n",
    "    for i, predictor in enumerate(top_predictors):\n",
    "        # Create the PartialDependenceDisplay object\n",
    "        pdp_display = PartialDependenceDisplay.from_estimator(\n",
    "            estimator=rf,\n",
    "            X=X_train,\n",
    "            features=[predictor],\n",
    "            response_method=\"auto\",\n",
    "            n_jobs=9,\n",
    "            n_cols=5,\n",
    "            ax=axes[i // 5, i % 5],\n",
    "        )\n",
    "        # Plot the PDP\n",
    "        # pdp_display.plot()\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    fig.savefig(f\"{dir_rf}/pdp_plot.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Compare predicted and actual values for both, train and test\n",
    "    print(f\" - Modobs...\")\n",
    "    model_evaluation_regression(\n",
    "        rf, X_train, y_train, X_test, y_test, save_directory=dir_rf\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an identical copy of the data but replace every variable with random values within the same range\n",
    "df_random = df_target.copy()\n",
    "for col in df_random.columns:\n",
    "    df_random[col] = np.random.uniform(\n",
    "        low=df_target[col].min(), high=df_target[col].max(), size=len(df_random)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data RF\n",
      " - Prepare data...\n",
      " - Fit random forest...\n",
      " - Feature Importance...\n",
      " - Partial Dependence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pascal/repos/padasch/ifn_analysis/ifna-venv/lib/python3.10/site-packages/sklearn/inspection/_plot/partial_dependence.py:972: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n",
      "  ax.set_ylim([min_val, max_val])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Modobs...\n",
      "Random Data RF\n",
      " - Prepare data...\n",
      " - Fit random forest...\n",
      " - Feature Importance...\n",
      " - Partial Dependence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pascal/repos/padasch/ifn_analysis/ifna-venv/lib/python3.10/site-packages/sklearn/inspection/_plot/partial_dependence.py:972: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n",
      "  ax.set_ylim([min_val, max_val])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Modobs...\n"
     ]
    }
   ],
   "source": [
    "# Run random forest on original data\n",
    "print(\"Original Data RF\")\n",
    "run_rf(df_target, target_of_interest, f\"{dir_rf}_predictors\")\n",
    "\n",
    "# Run random forest on random data\n",
    "print(\"Random Data RF\")\n",
    "run_rf(df_random, target_of_interest, f\"{dir_rf}_random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pascal/anaconda3/lib/python3.10/subprocess.py:1072: ResourceWarning: subprocess 22192 is still running\n",
      "  _warn(\"subprocess %s is still running\" % self.pid,\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chime\u001b[38;5;241m.\u001b[39msuccess()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Stop here"
     ]
    }
   ],
   "source": [
    "chime.success()\n",
    "raise ValueError(\"Stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Code Archive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corr_and_order_cols(df, target_of_interest):\n",
    "    \"\"\"\n",
    "    Calculate correlation between target of interest and all other variables.\n",
    "    Order variables by correlation.\n",
    "    \"\"\"\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Sort columns by correlation with the target variable\n",
    "    sorted_columns = corr[target_of_interest].sort_values(ascending=False).index\n",
    "\n",
    "    # Order df by correlation with the target variable\n",
    "    df = df[sorted_columns]\n",
    "\n",
    "    # Return sorted df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/correlations_growth_and_mortality.png` does not exist. Creating...\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ! Heatmap for growth and mortality variables\n",
    "# Get filename\n",
    "filename = f\"{dir_heatmap_growhtmort}/correlations_growth_and_mortality.png\"\n",
    "# Skip if file already exists\n",
    "if os.path.exists(filename):\n",
    "    print(f\" ✅ File `{filename}` already exists.\")\n",
    "else:\n",
    "    print(f\" ❌ File `{filename}` does not exist. Creating...\")\n",
    "    # Order cols by correlation with target variable\n",
    "    df_hm = calculate_corr_and_order_cols(df, target_of_interest)\n",
    "\n",
    "    # Make a correlation heatmap of df\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    sns.heatmap(\n",
    "        df_hm.corr(),\n",
    "        annot=True,\n",
    "        cmap=\"RdBu\",\n",
    "        cbar=True,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "    )\n",
    "    plt.title(\"Correlations of Growth and Mortality Variables\")\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.close()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_1.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_2.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_3.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_4.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_5.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_6.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_7.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_8.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_9.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_10.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_11.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_12.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_13.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_14.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_15.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_16.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_17.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_18.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_19.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_20.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_21.png` does not exist. Creating...\n",
      " ❌ File `correlation_exploration/species_quercus-heightall/mort_nat_vol_prc_yr/edo/heatmaps/correlations_22.png` does not exist. Creating...\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ! Heatmaps for target and predictor variables\n",
    "# Sort columns by correlation with target variable\n",
    "df_merged = calculate_corr_and_order_cols(df_merged, target_of_interest)\n",
    "\n",
    "# Make heatmaps of 15 variables at a time to keep readability\n",
    "map_counter = 0\n",
    "for i in range(0, len(df_merged.columns), 15):\n",
    "    # Check if file exists already\n",
    "    map_counter += 1\n",
    "    filename = f\"{dir_heatmap_predictors}/correlations_{map_counter}.png\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\" ✅ File `{filename}` already exists.\")\n",
    "    else:\n",
    "        print(f\" ❌ File `{filename}` does not exist. Creating...\")\n",
    "        # Plot file\n",
    "        # Reorder columns with target_of_interest at first position\n",
    "        cols = [target_of_interest] + df_merged.columns[i : i + 15].tolist()\n",
    "        df_subset = df_merged[cols]\n",
    "\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        sns.heatmap(\n",
    "            df_subset.corr(),\n",
    "            annot=True,\n",
    "            cmap=\"RdBu\",\n",
    "            cbar=True,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "        )\n",
    "        plt.title(f\"Correlations of Target {target_of_interest} and Predictors\")\n",
    "        plt.tight_layout()  # Adjust figure layout\n",
    "        plt.savefig(\n",
    "            filename, dpi=300, bbox_inches=\"tight\"\n",
    "        )  # Save the entire figure without cutting off\n",
    "\n",
    "        plt.close()\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation bar chart\n",
    "# For each variable in the dataset, calculate the correlation with the target variable\n",
    "corr = df_merged.corr()[target_of_interest].sort_values(ascending=False)\n",
    "# Remove the target variable from the list\n",
    "corr = corr.drop(target_of_interest)\n",
    "\n",
    "# Plot the correlations\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(f\"Correlation of {target_of_interest} with Predictors\")\n",
    "plt.xlabel(\"Predictor\")\n",
    "plt.ylabel(\"Pearson r\")\n",
    "plt.bar(corr.index, corr.values, color=\"RdBu\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig(f\"{dir_base}/correlation_bar_chart.png\", dpi=300)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifna-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
