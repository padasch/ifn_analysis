{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings for Download: ERA5 Hourly\n",
    "\n",
    "[Dataset](https://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_LAND_HOURLY#description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADJUST ONLY THESE PARAMETERS!\n",
    "\n",
    "# Debug settings\n",
    "skip_to_i = 0 # 0 = no skip | If loop broke, resume by setting this i to where you want to continue\n",
    "first_5000_sites = False\n",
    "\n",
    "# Timescale of Interest\n",
    "my_plus_years   = 5 # Years to add after first visit\n",
    "my_minus_years  = 2 # Years to add before first visit\n",
    "my_first_date   = \"-01-01\"  # First date of first year \"-MM-DD\"\n",
    "my_last_date    = \"-12-31\"  # Last date of last year \"-MM-DD\"\n",
    "\n",
    "# Dataset of Interest\n",
    "product            = \"ECMWF/ERA5_LAND/HOURLY\"\n",
    "product_start_date = \"1950-01-01\"\n",
    "product_end_date   = \"2023-10-23\"\n",
    "product_scale      = 11132\n",
    "\n",
    "output_scale       = product_scale\n",
    "output_folder      = \"gee-raw-data/era5-hourly\"\n",
    "\n",
    "my_bands = [\n",
    "    \"dewpoint_temperature_2m\" # Needed to back-calculate humidity\n",
    "    ,\"temperature_2m\"         # Needed to back-calculate humidity\n",
    "    ,\"skin_temperature\"\n",
    "    # ,\"soil_temperature_level_1\"\n",
    "    # ,\"soil_temperature_level_2\"\n",
    "    # ,\"soil_temperature_level_3\"\n",
    "    # ,\"soil_temperature_level_4\"\n",
    "    # ,\"lake_bottom_temperature\"\n",
    "    # ,\"lake_ice_depth\"\n",
    "    # ,\"lake_ice_temperature\"\n",
    "    # ,\"lake_mix_layer_depth\"\n",
    "    # ,\"lake_mix_layer_temperature\"\n",
    "    # ,\"lake_shape_factor\"\n",
    "    # ,\"lake_total_layer_temperature\"\n",
    "    # ,\"snow_albedo\"\n",
    "    # ,\"snow_cover\"\n",
    "    # ,\"snow_density\"\n",
    "    # ,\"snow_depth\"\n",
    "    # ,\"snow_depth_water_equivalent\"\n",
    "    ,\"snowfall\"\n",
    "    # ,\"snowmelt\"\n",
    "    # ,\"temperature_of_snow_layer\"\n",
    "    # ,\"skin_reservoir_content\"\n",
    "    ,\"volumetric_soil_water_layer_1\"\n",
    "    ,\"volumetric_soil_water_layer_2\"\n",
    "    ,\"volumetric_soil_water_layer_3\"\n",
    "    ,\"volumetric_soil_water_layer_4\"\n",
    "    # ,\"forecast_albedo\"\n",
    "    # ,\"surface_latent_heat_flux\"\n",
    "    # ,\"surface_net_solar_radiation\"\n",
    "    # ,\"surface_net_thermal_radiation\"\n",
    "    # ,\"surface_sensible_heat_flux\"\n",
    "    # ,\"surface_solar_radiation_downwards\"\n",
    "    # ,\"surface_thermal_radiation_downwards\"\n",
    "    # ,\"evaporation_from_bare_soil\"\n",
    "    # ,\"evaporation_from_open_water_surfaces_excluding_oceans\"\n",
    "    # ,\"evaporation_from_the_top_of_canopy\"\n",
    "    # ,\"evaporation_from_vegetation_transpiration\"\n",
    "    ,\"potential_evaporation\"\n",
    "    # ,\"runoff\"\n",
    "    # ,\"snow_evaporation\"\n",
    "    # ,\"sub_surface_runoff\"\n",
    "    # ,\"surface_runoff\"\n",
    "    ,\"total_evaporation\"\n",
    "    ,\"u_component_of_wind_10m\"\n",
    "    ,\"v_component_of_wind_10m\"\n",
    "    ,\"surface_pressure\"                 # Needed to back-calculate humidity\n",
    "    ,\"total_precipitation\"\n",
    "    ,\"leaf_area_index_high_vegetation\"\n",
    "    ,\"leaf_area_index_low_vegetation\"\n",
    "    \n",
    "    # Hourly variables below are created by GEE people and are not used here\n",
    "    # ,\"snowfall_hourly\"\n",
    "    # ,\"snowmelt_hourly\"\n",
    "    # ,\"surface_latent_heat_flux_hourly\"\n",
    "    # ,\"surface_net_solar_radiation_hourly\"\n",
    "    # ,\"surface_net_thermal_radiation_hourly\"\n",
    "    # ,\"surface_sensible_heat_flux_hourly\"\n",
    "    # ,\"surface_solar_radiation_downwards_hourly\"\n",
    "    # ,\"surface_thermal_radiation_downwards_hourly\"\n",
    "    # ,\"evaporation_from_bare_soil_hourly\"\n",
    "    # ,\"evaporation_from_open_water_surfaces_excluding_oceans_hourly\"\n",
    "    # ,\"evaporation_from_the_top_of_canopy_hourly\"\n",
    "    # ,\"evaporation_from_vegetation_transpiration_hourly\"\n",
    "    # ,\"potential_evaporation_hourly\"\n",
    "    # ,\"runoff_hourly\"\n",
    "    # ,\"snow_evaporation_hourly\"\n",
    "    # ,\"sub_surface_runoff_hourly\"\n",
    "    # ,\"surface_runoff_hourly\"\n",
    "    # ,\"total_evaporation_hourly\"\n",
    "    # ,\"total_precipitation_hourly\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Routine Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i gee_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import ee\n",
    "# ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "import os, re\n",
    "import pandas as pd\n",
    "from gee_subset import gee_subset\n",
    "import geopandas as gpd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_location_site_df()\n",
    "data_clean = adjust_first_last_date(\n",
    "    df = data,\n",
    "    plus_years  = my_plus_years,\n",
    "    minus_years = my_minus_years,\n",
    "    first_date  = my_first_date,\n",
    "    last_date   = my_last_date,\n",
    ")\n",
    "\n",
    "if first_5000_sites:\n",
    "    data_clean = data_clean.iloc[0:10, ]\n",
    "\n",
    "cnt     = len(data_clean)\n",
    "siteSet = list(range(0, cnt, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to download data per year and band for ERA5 daily.\n",
    "This is needed to avoid reaching user memory limit.\n",
    "'''\n",
    "current_year = 2013\n",
    "output_folder_yr = output_folder + '/' + str(current_year)\n",
    "current_start = str(current_year) + '-01-01'\n",
    "current_end   = str(current_year + 1) + '-01-01'\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "# output_folder_yr_band = output_folder_yr + '/' + band\n",
    "if not os.path.exists(output_folder_yr):\n",
    "    os.makedirs(output_folder_yr)\n",
    "\n",
    "# Start loop over bands\n",
    "for band in my_bands:\n",
    "        \n",
    "    # print('\\014 Working on: ', band)\n",
    "        \n",
    "    # Start loop over all the data\n",
    "    for i in siteSet:\n",
    "        \n",
    "        # Check if site needs data from current year\n",
    "        i_year_0 = datetime.strptime(data_clean.iloc[i, 3], \"%Y-%m-%d\").year\n",
    "        i_year_1 = datetime.strptime(data_clean.iloc[i, 4], \"%Y-%m-%d\").year\n",
    "        \n",
    "        if  i_year_0 <= current_year <= i_year_1:\n",
    "        \n",
    "            # Skip to site i, if required\n",
    "            if i < skip_to_i:\n",
    "                continue\n",
    "        \n",
    "            df_loop = gee_subset.gee_subset(\n",
    "                product    = product, \n",
    "                bands      = [band], \n",
    "                start_date = larger_date(current_start, product_start_date), \n",
    "                end_date   = smaller_date(current_end, product_end_date), \n",
    "                latitude   = data_clean.iloc[i, 2], \n",
    "                longitude  = data_clean.iloc[i, 1], \n",
    "                scale      = max(product_scale, output_scale)\n",
    "                )\n",
    "\n",
    "            sid = str(data_clean.iloc[i, 0]) \n",
    "            df_loop[\"SiteID\"] = sid\n",
    "            df_loop = df_loop.drop(columns=['id', 'longitude', 'latitude', 'product'])\n",
    "            df_loop.to_csv(output_folder_yr + '/' + band + \"_site_\" + str(data_clean.iloc[i, 0]) + \".csv\")\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Acess via [0, 1]: \", data_clean.columns[1], \"\\t = \", data_clean.iloc[0, 1])\n",
    "# print(\"Acess via [0, 2]: \", data_clean.columns[2], \"\\t = \", data_clean.iloc[0, 2])\n",
    "# print(\"Acess via [0, 3]: \", data_clean.columns[3], \"\\t = \", data_clean.iloc[0, 3])\n",
    "# print(\"Acess via [0, 4]: \", data_clean.columns[4], \"\\t = \", data_clean.iloc[0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2008, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2009, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2010, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2011, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2012, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2013, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2014, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2015, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2016, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2017, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2018, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2019, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2020, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_for_year(2021, data_clean, siteSet, output_folder, product, my_bands, product_scale, output_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9 s ± 265 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "for i in siteSet:\n",
    "\n",
    "    if i < skip_to_i:\n",
    "        continue\n",
    "    \n",
    "    df_loop = gee_subset.gee_subset(\n",
    "        product    = product, \n",
    "        bands      = my_bands, \n",
    "        start_date = larger_date(data_clean.iloc[i, 3],  product_start_date), \n",
    "        end_date   = smaller_date(data_clean.iloc[i, 4], product_end_date), \n",
    "        latitude   = data_clean.iloc[i, 2], \n",
    "        longitude  = data_clean.iloc[i ,1], \n",
    "        scale      = max(product_scale, output_scale)\n",
    "        )\n",
    "\n",
    "    # Attach site id for joining later\n",
    "    df_loop[\"SiteID\"] = str(data_clean.iloc[i, 0]) \n",
    "    \n",
    "    # Remove unnecessary columns to save space and time\n",
    "    df_loop = df_loop.drop(columns=['id', 'longitude', 'latitude', 'product'])\n",
    "    \n",
    "    # Write to csv file\n",
    "    df_loop.to_csv(output_folder + \"/site_\" + str(data_clean.iloc[i, 0]) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9 s ± 608 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "for i in siteSet:\n",
    "\n",
    "    if i < skip_to_i:\n",
    "        continue\n",
    "    \n",
    "    df_loop = gee_subset.gee_subset(\n",
    "        product    = product, \n",
    "        bands      = my_bands, \n",
    "        start_date = larger_date(data_clean.iloc[i, 3],  product_start_date), \n",
    "        end_date   = smaller_date(data_clean.iloc[i, 4], product_end_date), \n",
    "        latitude   = data_clean.iloc[i, 2], \n",
    "        longitude  = data_clean.iloc[i ,1], \n",
    "        scale      = max(product_scale, output_scale)\n",
    "        )\n",
    "\n",
    "    # Attach site id for joining later\n",
    "    df_loop[\"SiteID\"] = str(data_clean.iloc[i, 0]) \n",
    "    \n",
    "    # Remove unnecessary columns to save space and time\n",
    "    df_loop = df_loop.drop(columns=['id', 'longitude', 'latitude', 'product'])\n",
    "    \n",
    "    # Write to csv file\n",
    "    df_loop.to_pickle(output_folder + \"/site_\" + str(data_clean.iloc[i, 0]) + \"topickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.4 s ± 758 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "for i in siteSet:\n",
    "\n",
    "    if i < skip_to_i:\n",
    "        continue\n",
    "    \n",
    "    df_loop = gee_subset.gee_subset(\n",
    "        product    = product, \n",
    "        bands      = my_bands, \n",
    "        start_date = larger_date(data_clean.iloc[i, 3],  product_start_date), \n",
    "        end_date   = smaller_date(data_clean.iloc[i, 4], product_end_date), \n",
    "        latitude   = data_clean.iloc[i, 2], \n",
    "        longitude  = data_clean.iloc[i ,1], \n",
    "        scale      = max(product_scale, output_scale)\n",
    "        )\n",
    "\n",
    "    # Attach site id for joining later\n",
    "    df_loop[\"SiteID\"] = str(data_clean.iloc[i, 0]) \n",
    "    \n",
    "    # Remove unnecessary columns to save space and time\n",
    "    df_loop = df_loop.drop(columns=['id', 'longitude', 'latitude', 'product'])\n",
    "    \n",
    "    # Write to csv file\n",
    "    df_loop.to_parquet(output_folder + \"/site_\" + str(data_clean.iloc[i, 0]) + \"toparquet.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.1 s ± 280 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "for i in siteSet:\n",
    "\n",
    "    if i < skip_to_i:\n",
    "        continue\n",
    "    \n",
    "    df_loop = gee_subset.gee_subset(\n",
    "        product    = product, \n",
    "        bands      = my_bands, \n",
    "        start_date = larger_date(data_clean.iloc[i, 3],  product_start_date), \n",
    "        end_date   = smaller_date(data_clean.iloc[i, 4], product_end_date), \n",
    "        latitude   = data_clean.iloc[i, 2], \n",
    "        longitude  = data_clean.iloc[i ,1], \n",
    "        scale      = max(product_scale, output_scale)\n",
    "        )\n",
    "\n",
    "    # Attach site id for joining later\n",
    "    df_loop[\"SiteID\"] = str(data_clean.iloc[i, 0]) \n",
    "    \n",
    "    # Remove unnecessary columns to save space and time\n",
    "    df_loop = df_loop.drop(columns=['id', 'longitude', 'latitude', 'product'])\n",
    "    \n",
    "    # Write to csv file\n",
    "    df_loop.to_feather(output_folder + \"/site_\" + str(data_clean.iloc[i, 0]) + \"tofeather.feather\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IFNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
